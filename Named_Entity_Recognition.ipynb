{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING SPACY LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: spacy in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.17.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Apple Company Found By Steve Jobs Is Looking At Buying U.K. Startup For $1 Billion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Company | ORG | Companies, agencies, institutions, etc.\n",
      "Steve Jobs | PERSON | People, including fictional\n",
      "$1 Billion | MONEY | Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.text} | {ent.label_} | {spacy.explain(ent.label_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy works with span(a substring, like [2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple Company"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "s1 = Span(doc, 0, 1, label='ORG')\n",
    "s2 = Span(doc, 4, 6, label='PERSON')\n",
    "\n",
    "doc.set_ents([s1, s2], default='unmodified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Company | ORG | Companies, agencies, institutions, etc.\n",
      "None\n",
      "Steve Jobs | PERSON | People, including fictional\n",
      "None\n",
      "$1 Billion | MONEY | Monetary values, including unit\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for en in doc.ents:\n",
    "    print(print(f'{en.text} | {en.label_} | {spacy.explain(en.label_)}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING NLTK LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import ne_chunk # for named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vidya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # used for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vidya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger') # pretrained model for POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Apple Company Found By Steve Jobs Is Looking At Buying U.K. Startup For $1 Billion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple',\n",
       " 'Company',\n",
       " 'Found',\n",
       " 'By',\n",
       " 'Steve',\n",
       " 'Jobs',\n",
       " 'Is',\n",
       " 'Looking',\n",
       " 'At',\n",
       " 'Buying',\n",
       " 'U.K.',\n",
       " 'Startup',\n",
       " 'For',\n",
       " '$',\n",
       " '1',\n",
       " 'Billion']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple', 'NNP'),\n",
       " ('Company', 'NNP'),\n",
       " ('Found', 'NNP'),\n",
       " ('By', 'IN'),\n",
       " ('Steve', 'NNP'),\n",
       " ('Jobs', 'NNP'),\n",
       " ('Is', 'VBZ'),\n",
       " ('Looking', 'VBG'),\n",
       " ('At', 'IN'),\n",
       " ('Buying', 'NNP'),\n",
       " ('U.K.', 'NNP'),\n",
       " ('Startup', 'NNP'),\n",
       " ('For', 'IN'),\n",
       " ('$', '$'),\n",
       " ('1', 'CD'),\n",
       " ('Billion', 'NNP')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = nltk.pos_tag(tokens)\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\vidya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker') # used in conjunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vidya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words') # word list corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Apple/NNP)\n",
      "  (ORGANIZATION Company/NNP Found/NNP By/IN Steve/NNP Jobs/NNP)\n",
      "  Is/VBZ\n",
      "  Looking/VBG\n",
      "  At/IN\n",
      "  (ORGANIZATION Buying/NNP)\n",
      "  U.K./NNP\n",
      "  Startup/NNP\n",
      "  For/IN\n",
      "  $/$\n",
      "  1/CD\n",
      "  Billion/NNP)\n"
     ]
    }
   ],
   "source": [
    "named_entities = nltk.ne_chunk(tag)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Trained Model (HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (4.42.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.24.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.24.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-ORG', 'score': 0.9483636, 'index': 1, 'word': 'Apple', 'start': 0, 'end': 5}, {'entity': 'B-PER', 'score': 0.9989231, 'index': 5, 'word': 'Steve', 'start': 23, 'end': 28}, {'entity': 'B-LOC', 'score': 0.99945205, 'index': 11, 'word': 'U', 'start': 55, 'end': 56}, {'entity': 'I-LOC', 'score': 0.9969195, 'index': 12, 'word': '.', 'start': 56, 'end': 57}, {'entity': 'I-LOC', 'score': 0.99891376, 'index': 13, 'word': 'K', 'start': 57, 'end': 58}, {'entity': 'I-LOC', 'score': 0.8938563, 'index': 14, 'word': '.', 'start': 58, 'end': 59}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('dslim/bert-base-NER')\n",
    "model = AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')\n",
    "\n",
    "nlp = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "example = 'Apple company found by Steve jobs is looking at buying U.K. startup for $1 billion'\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dslim/bert-base-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 750.03 examples/s]\n",
      "C:\\Users\\vidya\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Word index 11 is out of range for label list of length 11.\n",
      "Warning: Word index 10 is out of range for label list of length 10.\n",
      "Warning: Word index 11 is out of range for label list of length 10.\n",
      "Warning: Word index 11 is out of range for label list of length 11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      " 33%|███▎      | 1/3 [00:28<00:40, 20.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1551992893218994, 'eval_runtime': 8.3645, 'eval_samples_per_second': 0.717, 'eval_steps_per_second': 0.12, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      " 67%|██████▋   | 2/3 [00:56<00:24, 24.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9709361791610718, 'eval_runtime': 8.6555, 'eval_samples_per_second': 0.693, 'eval_steps_per_second': 0.116, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 3/3 [01:25<00:00, 28.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8823868036270142, 'eval_runtime': 8.2736, 'eval_samples_per_second': 0.725, 'eval_steps_per_second': 0.121, 'epoch': 3.0}\n",
      "{'train_runtime': 85.7779, 'train_samples_per_second': 0.21, 'train_steps_per_second': 0.035, 'train_loss': 2.2174530029296875, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=2.2174530029296875, metrics={'train_runtime': 85.7779, 'train_samples_per_second': 0.21, 'train_steps_per_second': 0.035, 'total_flos': 4703596756992.0, 'train_loss': 2.2174530029296875, 'epoch': 3.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "label_list = ['O', 'B-ORG', 'I-ORG', 'B-LOC', 'B-MONEY', 'I-MONEY', 'B-PERSON', 'I-PERSON']\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'dslim/bert-base-NER',\n",
    "    num_labels=len(label_list),\n",
    "    ignore_mismatched_sizes=True,\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    label2id={label: i for i, label in enumerate(label_list)}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Example dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    'id': ['0', '1', '2', '3', '4', '5'],\n",
    "    'text': [\n",
    "        ['Apple', 'company', 'founded', 'by', 'Steve', 'Jobs', 'is', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.'],\n",
    "        ['Microsoft', 'Corporation', 'reported', 'revenue', 'of', '$', '100', 'million', 'last', 'quarter', '.'],\n",
    "        ['John', 'Doe', 'donated', '$', '500', 'to', 'charity', '.'],\n",
    "        ['Google', 'LLC', 'announced', 'a', 'new', 'partnership', 'with', 'Samsung', 'in', 'New', 'York', '.'],\n",
    "        ['Elon', 'Musk', 'tweeted', 'about', 'the', 'SpaceX', 'launch', 'scheduled', 'for', 'next', 'month', '.'],\n",
    "        ['Amazon', 'Prime', 'members', 'get', 'free', 'shipping', 'on', 'orders', 'over', '$', '25', '.']\n",
    "    ],\n",
    "    'labels': [\n",
    "        [1, 2, 0, 0, 6, 6, 0, 0, 3, 0, 0, 4, 5, 5, 0],\n",
    "        [1, 2, 0, 0, 0, 4, 5, 5, 0, 0, 0],\n",
    "        [6, 7, 0, 4, 5, 0, 1, 0],\n",
    "        [1, 2, 0, 0, 0, 0, 0, 3, 0, 4, 4],\n",
    "        [1, 6, 0, 0, 0, 0, 0, 7, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0]\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "label_all_tokens = True  # Whether to label all tokens of a split word or just the first token\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['text'], truncation=True, is_split_into_words=True, padding='max_length')\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Add check for out-of-bounds labels\n",
    "                if word_idx < len(label):\n",
    "                    label_value = label[word_idx]\n",
    "                    if 0 <= label_value < len(label_list):\n",
    "                        label_ids.append(label_value)\n",
    "                    else:\n",
    "                        print(f'Warning: Label {label_value} at word index {word_idx} exceeds label_list length.')\n",
    "                        label_ids.append(-100)\n",
    "                else:\n",
    "                    print(f'Warning: Word index {word_idx} is out of range for label list of length {len(label)}.')\n",
    "                    label_ids.append(-100)  # Default value if out of range\n",
    "            else:\n",
    "                # For subword tokens\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,  # Normally, you would have a separate validation set\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Documents\\\\tokenizer_config.json',\n",
       " './Documents\\\\special_tokens_map.json',\n",
       " './Documents\\\\vocab.txt',\n",
       " './Documents\\\\added_tokens.json',\n",
       " './Documents\\\\tokenizer.json')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./Documents')\n",
    "tokenizer.save_pretrained('./Documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./Documents')\n",
    "model = AutoModelForTokenClassification.from_pretrained('./Documents')\n",
    "\n",
    "# Create a NER pipeline\n",
    "ner_pipeline = pipeline('ner', model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Amazon, Label: B-ORG, Confidence: 0.2337\n",
      "Entity: plans, Label: I-ORG, Confidence: 0.1689\n",
      "Entity: to, Label: B-MONEY, Confidence: 0.1672\n",
      "Entity: invest, Label: B-MONEY, Confidence: 0.1836\n",
      "Entity: $, Label: B-MONEY, Confidence: 0.1888\n",
      "Entity: 2, Label: B-MONEY, Confidence: 0.1977\n",
      "Entity: billion, Label: B-MONEY, Confidence: 0.1797\n",
      "Entity: in, Label: I-PERSON, Confidence: 0.1709\n",
      "Entity: India, Label: B-PERSON, Confidence: 0.2040\n",
      "Entity: ., Label: I-PERSON, Confidence: 0.1821\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "example_sentence = 'Amazon plans to invest $2 billion in India.'\n",
    "\n",
    "# Use the NER pipeline to predict entities\n",
    "ner_results = ner_pipeline(example_sentence)\n",
    "\n",
    "# Print the results\n",
    "for entity in ner_results:\n",
    "    print(f'Entity: {entity['word']}, Label: {entity['entity']}, Confidence: {entity['score']:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(ner_results[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-ORG',\n",
       "  'score': 0.2336563,\n",
       "  'index': 1,\n",
       "  'word': 'Amazon',\n",
       "  'start': 0,\n",
       "  'end': 6},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.16890727,\n",
       "  'index': 2,\n",
       "  'word': 'plans',\n",
       "  'start': 7,\n",
       "  'end': 12},\n",
       " {'entity': 'B-MONEY',\n",
       "  'score': 0.16719468,\n",
       "  'index': 3,\n",
       "  'word': 'to',\n",
       "  'start': 13,\n",
       "  'end': 15},\n",
       " {'entity': 'B-MONEY',\n",
       "  'score': 0.18360063,\n",
       "  'index': 4,\n",
       "  'word': 'invest',\n",
       "  'start': 16,\n",
       "  'end': 22},\n",
       " {'entity': 'B-MONEY',\n",
       "  'score': 0.1887774,\n",
       "  'index': 5,\n",
       "  'word': '$',\n",
       "  'start': 23,\n",
       "  'end': 24},\n",
       " {'entity': 'B-MONEY',\n",
       "  'score': 0.19770099,\n",
       "  'index': 6,\n",
       "  'word': '2',\n",
       "  'start': 24,\n",
       "  'end': 25},\n",
       " {'entity': 'B-MONEY',\n",
       "  'score': 0.1797492,\n",
       "  'index': 7,\n",
       "  'word': 'billion',\n",
       "  'start': 26,\n",
       "  'end': 33},\n",
       " {'entity': 'I-PERSON',\n",
       "  'score': 0.17087922,\n",
       "  'index': 8,\n",
       "  'word': 'in',\n",
       "  'start': 34,\n",
       "  'end': 36},\n",
       " {'entity': 'B-PERSON',\n",
       "  'score': 0.2039539,\n",
       "  'index': 9,\n",
       "  'word': 'India',\n",
       "  'start': 37,\n",
       "  'end': 42},\n",
       " {'entity': 'I-PERSON',\n",
       "  'score': 0.18214251,\n",
       "  'index': 10,\n",
       "  'word': '.',\n",
       "  'start': 42,\n",
       "  'end': 43}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" model_label_list = ['O', 'B-ORG', 'I-ORG', 'B-LOC', 'B-MONEY', 'I-MONEY', 'B-PERSON', 'I-PERSON']\\n\\nfor result in ner_results:\\n    # Extract label index\\n    label_index = int(result['entity'].split('_')[1])  # Extract the index part\\n    # Map to label name\\n    label_name = model_label_list[label_index]\\n    print(f'Entity: {result['word']}, Label: {label_name}, Confidence: {result['score']:.4f}') \""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' model_label_list = ['O', 'B-ORG', 'I-ORG', 'B-LOC', 'B-MONEY', 'I-MONEY', 'B-PERSON', 'I-PERSON']\n",
    "\n",
    "for result in ner_results:\n",
    "    # Extract label index\n",
    "    label_index = int(result['entity'].split('_')[1])  # Extract the index part\n",
    "    # Map to label name\n",
    "    label_name = model_label_list[label_index]\n",
    "    print(f'Entity: {result['word']}, Label: {label_name}, Confidence: {result['score']:.4f}') '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" #label_list = ['O', 'B-ORG', 'I-ORG', 'B-LOC', 'B-MONEY', 'I-MONEY', 'B-PERSON', 'I-PERSON']\\n\\nfor i in ner_results:\\n    print(i['entity'][-1])\\n    print(label_list[int(i['entity'][-1])]) \""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' #label_list = ['O', 'B-ORG', 'I-ORG', 'B-LOC', 'B-MONEY', 'I-MONEY', 'B-PERSON', 'I-PERSON']\n",
    "\n",
    "for i in ner_results:\n",
    "    print(i['entity'][-1])\n",
    "    print(label_list[int(i['entity'][-1])]) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'everybody!', 'Hyundai', 'is', 'one', 'of', 'the', 'most', 'iconic', 'companies', 'to', 'enter', 'the', 'Indian', 'markets', 'It', 'is', 'hard', 'to', 'believe', 'that', 'a', 'South', 'Korean', 'brand,', 'which', 'was', 'once', 'a', 'difficult', 'name', 'to', 'pronounce,', 'is', 'today', 'about', 'to', 'launch', 'the', 'biggest', 'IPO', 'in', 'the', 'history', 'of', 'India', 'Indian', 'stock', 'markets', 'could', 'soon', 'see', 'their', 'biggest', 'ever', 'IPO', 'South', 'Korean', 'automaker', 'Hyundai', 'is', 'gearing', 'up', 'for', 'a', 'significant', 'milestone', 'by', 'going', 'public', 'in', 'India', \"Hyundai's\", 'IPO', 'is', 'poised', 'to', 'be', 'one', 'of', 'the', 'largest', 'in', 'recent', 'times,', 'potentially', 'the', 'biggest', 'IPO', 'ever', 'in', 'India', 'This', 'would', 'be', \"India's\", 'largest', 'IPO', 'and', 'the', 'first', 'van', 'automaker', 'since', 'Maruti', 'Suzuki', 'in', '2003In', '1996,', 'when', 'Hyundai', 'entered', 'the', 'Indian', 'market,', 'Maruti', 'was', 'the', 'most', 'dominant', 'player', 'with', 'a', '60%', 'market', 'share,', 'almost', 'a', 'monopoly', 'amongst', 'the', 'middle', 'class', 'in', 'India', 'Despite', 'this,', 'Hyundai', 'came', 'in', 'with', 'models', 'like', 'Santro,', 'i10,', 'i20,', 'and', 'Creta,', 'making', 'a', 'significant', 'impact', 'Today,', 'Hyundai', 'is', 'the', 'second-largest', 'automaker', 'in', 'the', 'country', 'with', 'a', 'profit', 'of', '4,653', 'crores', 'In', 'terms', 'of', 'profitability,', 'Hyundai', 'is', 'way', 'ahead', 'of', 'Maruti', 'In', 'FY23,', 'while', 'Hyundai', 'made', 'a', 'profit', 'of', '65,355', 'rupees', 'per', 'vehicle,', 'Maruti', 'made', 'a', 'profit', 'of', '4,939', 'rupees', 'per', 'vehicle', 'Now,', 'after', '28', 'years', 'of', 'establishing', 'a', 'stronghold', 'in', 'the', 'Indian', 'market,', 'Hyundai', 'plans', 'to', 'raise', '25,000', 'crores', 'with', 'its', 'IPO', 'Sounds', 'fantastic,', 'right?', 'However,', 'there', 'are', 'some', 'hidden', 'risks', 'that', 'Hyundai', 'faces', 'which', 'may', 'jeopardize', 'all', 'the', 'progress', 'theyve', 'made', 'in', 'India', 'All', 'of', 'this', 'is', 'detailed', 'in', 'a', '436-page', 'DRHP', 'paper', 'that', 'Hyundai', 'filed', 'with', 'SEBI', \"We've\", 'gone', 'through', 'all', 'this', 'data', 'so', 'that', 'you', 'can', 'sit', 'back', 'and', 'consume', 'the', 'most', 'important', 'bits', 'of', 'this', '400-page', 'document', 'as', 'easily', 'as', 'watching', 'a', 'movie', 'In', 'this', 'episode,', \"let's\", 'dive', 'into', 'understanding', 'how', 'a', 'South', 'Korean', 'company', 'like', 'Hyundai', 'broke', 'into', 'the', 'Indian', 'automobile', 'market,', 'leaving', 'behind', 'giants', 'like', 'Tata', 'and', 'Mahindra', 'Despite', 'being', 'a', 'foreign', 'company,', 'how', 'did', 'Hyundai', 'achieve', 'such', 'high', 'profitability', 'and', 'outpace', 'a', 'mammoth', 'like', 'Maruti', 'in', 'India?', 'Most', 'importantly,', 'after', 'making', 'such', 'amazing', 'progress,', 'what', 'are', 'the', 'hidden', 'risks', 'that', 'could', 'topple', \"Hyundai's\", 'growth', 'in', 'India?', 'Also,', 'a', 'quick', 'disclaimer:', 'I', 'am', 'not', 'a', 'SEBI-registered', 'investment', 'adviser,', 'and', 'this', 'video', 'is', 'not', 'meant', 'to', 'give', 'you', 'investment', 'advice', 'but', 'to', 'only', 'educate', 'you', 'about', 'the', 'rise', 'of', 'Hyundai', 'in', 'India', 'Before', 'we', 'move', 'on,', 'I', 'want', 'to', 'quickly', 'introduce', 'you', 'to', 'our', 'education', 'partners', 'of', \"today's\", 'episode,', 'Scaler', 'School', 'of', 'Business', 'They', 'are', 'bringing', 'something', 'absolutely', 'revolutionary', 'If', \"you're\", 'seeking', 'business', 'education,', 'Scaler', 'is', 'offering', 'a', 'full-time', 'on-campus', '18-month', 'PG', 'program', 'in', 'management', 'and', 'technology', 'This', 'program', 'is', 'designed', 'by', 'leaders', 'who', 'have', 'built', 'billion-dollar', 'businesses', 'like', 'Uber,', 'Myntra,', 'and', 'Meesho', 'Today,', 'legacy', 'college', 'names', 'are', 'useless', 'without', 'competitive', 'skill', 'sets', 'This', 'is', 'why,', 'as', 'part', 'of', 'the', 'Scaler', 'School', 'of', 'Business', 'curriculum,', 'you', 'will', 'work', 'on', 'real-world', 'projects', 'sourced', 'from', 'real', 'companies', 'You', 'will', 'build', 'your', 'own', 'business,', 'take', 'it', 'to', 'the', 'market,', 'generate', 'revenue,', 'and', 'raise', 'VC', 'capital', 'while', 'studying', 'on', 'campus', 'Since', 'Scaler', 'has', 'been', 'in', 'the', 'education', 'industry', 'for', 'seven', 'long', 'years,', 'they', 'already', 'have', 'access', 'to', '1,200+', 'company', 'partners,', 'something', 'no', 'other', 'B-School', 'in', 'the', 'country', 'can', 'offer', 'right', 'now', 'For', 'their', 'online', 'programs,', \"they've\", 'seen', 'placement', 'rates', 'of', '96%', 'with', 'a', 'median', 'CTC', 'of', '25', 'lakh', 'rupees,', 'verified', 'by', 'B2K', 'Analytics,', 'the', 'same', 'partner', 'that', 'verifies', 'reports', 'for', 'IIMs', 'Now,', 'Scaler', 'School', 'of', 'Business', 'is', 'handpicking', 'only', '75', 'students', 'for', 'their', 'founding', 'cohort', 'starting', 'in', 'August', '2024', 'They', 'have', 'scholarships', 'up', 'to', '100%,', 'and', 'applications', 'are', 'open', 'right', 'now', 'If', 'you', 'want', 'to', 'become', 'a', 'brilliant', 'business', 'leader,', 'apply', 'for', 'Scaler', 'School', 'of', 'Business', 'using', 'the', 'link', 'in', 'my', 'description', 'and', 'in', 'the', 'comment', 'section', 'Now,', 'on', 'with', 'the', 'episode', 'The', 'first', 'thing', 'you', 'need', 'to', 'understand', 'about', 'a', 'company', 'is', 'their', 'philosophy', 'of', 'success', 'In', 'this', 'case,', 'you', 'need', 'to', 'understand', 'what', 'exactly', 'is', 'the', 'philosophy', 'that', 'turned', 'Hyundai', 'into', 'what', 'it', 'is', 'today', 'in', 'India', 'Like', 'we', 'saw', 'in', 'the', 'Boeing', 'video,', 'no', 'matter', 'how', 'much', 'profitability,', 'EBITDA,', 'or', 'earnings', 'per', 'share', 'a', 'company', 'has,', 'if', 'it', 'deviates', 'from', 'its', 'core', 'philosophy,', 'all', 'these', 'numbers', 'will', 'go', 'for', 'a', 'toss', 'So', \"let's\", 'understand', 'the', 'core', 'philosophy', 'and', 'then', 'move', 'on', 'to', 'profitability,', 'EBITDA,', 'and', 'other', 'numbers', 'Now', 'the', 'question', 'is,', 'what', 'exactly', 'is', 'the', 'secret', 'recipe', 'for', \"Hyundai's\", 'success', 'in', 'India?', 'This', 'story', 'dates', 'back', 'to', 'the', 'early', '1990s', 'Until', 'this', 'point,', 'India', 'only', 'had', 'a', 'few', 'car', 'brands:', 'Hindustan', 'Ambassador,', 'Contessa,', 'Premier', 'Padmini,', 'Standard', 'Herald,', '2000,', 'Maruti', '800,', 'Omni,', 'and', 'Gypsy']\n"
     ]
    }
   ],
   "source": [
    "data = \"Hi everybody! Hyundai is one of the most iconic companies to enter the Indian markets. It is hard to believe that a South Korean brand, which was once a difficult name to pronounce, is today about to launch the biggest IPO in the history of India. Indian stock markets could soon see their biggest ever IPO. South Korean automaker Hyundai is gearing up for a significant milestone by going public in India. Hyundai's IPO is poised to be one of the largest in recent times, potentially the biggest IPO ever in India. This would be India's largest IPO and the first van automaker since Maruti Suzuki in 2003.In 1996, when Hyundai entered the Indian market, Maruti was the most dominant player with a 60% market share, almost a monopoly amongst the middle class in India. Despite this, Hyundai came in with models like Santro, i10, i20, and Creta, making a significant impact. Today, Hyundai is the second-largest automaker in the country with a profit of 4,653 crores. In terms of profitability, Hyundai is way ahead of Maruti. In FY23, while Hyundai made a profit of 65,355 rupees per vehicle, Maruti made a profit of 4,939 rupees per vehicle. Now, after 28 years of establishing a stronghold in the Indian market, Hyundai plans to raise 25,000 crores with its IPO. Sounds fantastic, right? However, there are some hidden risks that Hyundai faces which may jeopardize all the progress theyve made in India. All of this is detailed in a 436-page DRHP paper that Hyundai filed with SEBI. We've gone through all this data so that you can sit back and consume the most important bits of this 400-page document as easily as watching a movie. In this episode, let's dive into understanding how a South Korean company like Hyundai broke into the Indian automobile market, leaving behind giants like Tata and Mahindra. Despite being a foreign company, how did Hyundai achieve such high profitability and outpace a mammoth like Maruti in India? Most importantly, after making such amazing progress, what are the hidden risks that could topple Hyundai's growth in India? Also, a quick disclaimer: I am not a SEBI-registered investment adviser, and this video is not meant to give you investment advice but to only educate you about the rise of Hyundai in India. Before we move on, I want to quickly introduce you to our education partners of today's episode, Scaler School of Business. They are bringing something absolutely revolutionary. If you're seeking business education, Scaler is offering a full-time on-campus 18-month PG program in management and technology. This program is designed by leaders who have built billion-dollar businesses like Uber, Myntra, and Meesho. Today, legacy college names are useless without competitive skill sets. This is why, as part of the Scaler School of Business curriculum, you will work on real-world projects sourced from real companies. You will build your own business, take it to the market, generate revenue, and raise VC capital while studying on campus. Since Scaler has been in the education industry for seven long years, they already have access to 1,200+ company partners, something no other B-School in the country can offer right now. For their online programs, they've seen placement rates of 96% with a median CTC of 25 lakh rupees, verified by B2K Analytics, the same partner that verifies reports for IIMs. Now, Scaler School of Business is handpicking only 75 students for their founding cohort starting in August 2024. They have scholarships up to 100%, and applications are open right now. If you want to become a brilliant business leader, apply for Scaler School of Business using the link in my description and in the comment section. Now, on with the episode. The first thing you need to understand about a company is their philosophy of success. In this case, you need to understand what exactly is the philosophy that turned Hyundai into what it is today in India. Like we saw in the Boeing video, no matter how much profitability, EBITDA, or earnings per share a company has, if it deviates from its core philosophy, all these numbers will go for a toss. So let's understand the core philosophy and then move on to profitability, EBITDA, and other numbers. Now the question is, what exactly is the secret recipe for Hyundai's success in India? This story dates back to the early 1990s. Until this point, India only had a few car brands: Hindustan Ambassador, Contessa, Premier Padmini, Standard Herald, 2000, Maruti 800, Omni, and Gypsy.\"\n",
    "data = data.replace('.','')\n",
    "text = data.split(' ')\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_labels = {\n",
    "    'Hyundai': 'B-ORG',\n",
    "    'India': 'B-LOC',\n",
    "    'South Korean': 'B-LOC',\n",
    "    'IPO': 'B-MISC',\n",
    "    'Maruti': 'B-ORG',\n",
    "    'SEBI': 'B-ORG',\n",
    "    'Scaler School of Business': 'B-ORG'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-ORG\n"
     ]
    }
   ],
   "source": [
    "print(entity_labels['Hyundai'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" labels = ['']*len(text)\\n\\n\\nfor i in range(len(text)):\\n    if text[i] in entity_labels:\\n        #print(text[i])\\n        labels[i] = entity_labels[text[i]]\\n    else:\\n        labels[i] = 'O'\\n        \\n\\nprint(labels) \""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' labels = ['']*len(text)\n",
    "\n",
    "\n",
    "for i in range(len(text)):\n",
    "    if text[i] in entity_labels:\n",
    "        #print(text[i])\n",
    "        labels[i] = entity_labels[text[i]]\n",
    "    else:\n",
    "        labels[i] = 'O'\n",
    "        \n",
    "\n",
    "print(labels) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "te = [\n",
    "    ['Hi', 'everybody!', 'Hyundai', 'is', 'one', 'of', 'the', 'most', 'iconic', 'companies', 'to', 'enter', 'the', 'Indian', 'markets'],\n",
    "    ['It', 'is', 'hard', 'to', 'believe', 'that', 'a', 'South', 'Korean', 'brand,', 'which', 'was', 'once', 'a', 'difficult', 'name', 'to', 'pronounce,', 'is', 'today', 'about', 'to', 'launch', 'the', 'biggest', 'IPO', 'in', 'the', 'history', 'of', 'India'],\n",
    "    ['Indian', 'stock', 'markets', 'could', 'soon', 'see', 'their', 'biggest', 'ever', 'IPO'],\n",
    "    ['South', 'Korean', 'automaker', 'Hyundai', 'is', 'gearing', 'up', 'for', 'a', 'significant', 'milestone', 'by', 'going', 'public', 'in', 'India'],\n",
    "    ['Hyundais', 'IPO', 'is', 'poised', 'to', 'be', 'one', 'of', 'the', 'largest', 'in', 'recent', 'times,', 'potentially', 'the', 'biggest', 'IPO', 'ever', 'in', 'India'],\n",
    "    ['This', 'would', 'be', 'Indias', 'largest', 'IPO', 'and', 'the', 'first', 'van', 'automaker', 'since', 'Maruti', 'Suzuki', 'in', '2003'],\n",
    "    ['In', '1996,', 'when', 'Hyundai', 'entered', 'the', 'Indian', 'market,', 'Maruti', 'was', 'the', 'most', 'dominant', 'player', 'with', 'a', '60%', 'market', 'share,', 'almost', 'a', 'monopoly', 'amongst', 'the', 'middle', 'class', 'in', 'India'],\n",
    "    ['Despite', 'this,', 'Hyundai', 'came', 'in', 'with', 'models', 'like', 'Santro,', 'i10,', 'i20,', 'and', 'Creta,', 'making', 'a', 'significant', 'impact'],\n",
    "    ['Today,', 'Hyundai', 'is', 'the', 'second-largest', 'automaker', 'in', 'the', 'country', 'with', 'a', 'profit', 'of', '4,653', 'crores'],\n",
    "    ['In', 'terms', 'of', 'profitability,', 'Hyundai', 'is', 'way', 'ahead', 'of', 'Maruti'],\n",
    "    ['In', 'FY23,', 'while', 'Hyundai', 'made', 'a', 'profit', 'of', '65,355', 'rupees', 'per', 'vehicle,', 'Maruti', 'made', 'a', 'profit', 'of', '4,939', 'rupees', 'per', 'vehicle'],\n",
    "    ['Now,', 'after', '28', 'years', 'of', 'establishing', 'a', 'stronghold', 'in', 'the', 'Indian', 'market,', 'Hyundai', 'plans', 'to', 'raise', '25,000', 'crores', 'with', 'its', 'IPO'],\n",
    "    ['Sounds', 'fantastic,', 'right?'],\n",
    "    ['However,', 'there', 'are', 'some', 'hidden', 'risks', 'that', 'Hyundai', 'faces', 'which', 'may', 'jeopardize', 'all', 'the', 'progress', 'theyve', 'made', 'in', 'India'],\n",
    "    ['All', 'of', 'this', 'is', 'detailed', 'in', 'a', '436-page', 'DRHP', 'paper', 'that', 'Hyundai', 'filed', 'with', 'SEBI'],\n",
    "    ['Weve', 'gone', 'through', 'all', 'this', 'data,', 'so', 'that', 'you', 'can', 'sit', 'back', 'and', 'consume', 'the', 'most', 'important', 'bits', 'of', 'this', '400-page', 'document', 'as', 'easily', 'as', 'watching', 'a', 'movie'],\n",
    "    ['In', 'this', 'episode,', 'lets', 'dive', 'into', 'understanding', 'how', 'a', 'South', 'Korean', 'company', 'like', 'Hyundai', 'broke', 'into', 'the', 'Indian', 'automobile', 'market,', 'leaving', 'behind', 'giants', 'like', 'Tata', 'and', 'Mahindra'],\n",
    "    ['Despite', 'being', 'a', 'foreign', 'company,', 'how', 'did', 'Hyundai', 'achieve', 'such', 'high', 'profitability', 'and', 'outpace', 'a', 'mammoth', 'like', 'Maruti', 'in', 'India?'],\n",
    "    ['Most', 'importantly,', 'after', 'making', 'such', 'amazing', 'progress,', 'what', 'are', 'the', 'hidden', 'risks', 'that', 'could', 'topple', 'Hyundai', 'growth', 'in', 'India?'],\n",
    "    ['Also,', 'a', 'quick', 'disclaimer:', 'I', 'am', 'not', 'a', 'SEBI','registered', 'investment', 'adviser,', 'and', 'this', 'video', 'is', 'not', 'meant', 'to', 'give', 'you', 'investment', 'advice', 'but', 'to', 'only', 'educate', 'you', 'about', 'the', 'rise', 'of', 'Hyundai', 'in', 'India'],\n",
    "    ['Before', 'we', 'move', 'on,', 'I', 'want', 'to', 'quickly', 'introduce', 'you', 'to', 'our', 'education', 'partners', 'of', 'todays', 'episode,', 'Scaler', 'School', 'of', 'Business'],\n",
    "    ['They', 'are', 'bringing', 'something', 'absolutely', 'revolutionary'],\n",
    "    ['If', 'youre', 'seeking', 'business', 'education,', 'Scaler', 'is', 'offering', 'a', 'full-time', 'on-campus', '18-month', 'PG', 'program', 'in', 'management', 'and', 'technology'],\n",
    "    ['This', 'program', 'is', 'designed', 'by', 'leaders', 'who', 'have', 'built', 'billion-dollar', 'businesses', 'like', 'Uber,', 'Myntra,', 'and', 'Meesho'],\n",
    "    ['Today,', 'legacy', 'college', 'names', 'are', 'useless', 'without', 'competitive', 'skill', 'sets'],\n",
    "    ['This', 'is', 'why,', 'as', 'part', 'of', 'the', 'Scaler', 'School', 'of', 'Business', 'curriculum,', 'you', 'will', 'work', 'on', 'real-world', 'projects', 'sourced', 'from', 'real', 'companies'],\n",
    "    ['You', 'will', 'build', 'your', 'own', 'business,', 'take', 'it', 'to', 'the', 'market,', 'generate', 'revenue,', 'and', 'raise', 'VC', 'capital', 'while', 'studying', 'on', 'campus'],\n",
    "    ['Since', 'Scaler', 'has', 'been', 'in', 'the', 'education', 'industry', 'for', 'seven', 'long', 'years,', 'they', 'already', 'have', 'access', 'to', '1,200+', 'company', 'partners,', 'something', 'no', 'other', 'B-School', 'in', 'the', 'country', 'can', 'offer', 'right', 'now'],\n",
    "    ['For', 'their', 'online', 'programs,', 'theyve', 'seen', 'placement', 'rates', 'of', '96%', 'with', 'a', 'median', 'CTC', 'of', '25', 'lakh', 'rupees,', 'verified', 'by', 'B2K', 'Analytics,', 'the', 'same', 'partner', 'that', 'verifies', 'reports', 'for', 'IIMs'],\n",
    "    ['Now,', 'Scaler', 'School', 'of', 'Business', 'is', 'handpicking', 'only', '75', 'students', 'for', 'their', 'founding', 'cohort', 'starting', 'in', 'August', '2024'],\n",
    "    ['They', 'have', 'scholarships', 'up', 'to', '100%,', 'and', 'applications', 'are', 'open', 'right', 'now', 'If', 'you', 'want', 'to', 'become', 'a', 'brilliant', 'business', 'leader,', 'apply', 'for', 'Scaler', 'School', 'of', 'Business', 'using', 'the', 'link', 'in', 'my', 'description', 'and', 'in', 'the', 'comment', 'section'],\n",
    "    ['Now,', 'on', 'with', 'the', 'episode'],\n",
    "    ['The', 'first', 'thing', 'you', 'need', 'to', 'understand', 'about', 'a', 'company', 'is', 'their', 'philosophy', 'of', 'success'],\n",
    "    ['In', 'this', 'case,', 'you', 'need', 'to', 'understand', 'what', 'exactly', 'is', 'the', 'philosophy', 'that', 'turned', 'Hyundai', 'into', 'what', 'it', 'is', 'today', 'in', 'India'],\n",
    "    ['Like', 'we', 'saw', 'in', 'the', 'Boeing', 'video', 'no', 'matter', 'how', 'much', 'profitability,', 'EBITDA,', 'or', 'earnings', 'per', 'share', 'a', 'company', 'has', 'if', 'it', 'deviates', 'from', 'its', 'core', 'philosophy', 'all', 'these', 'numbers', 'will', 'go', 'for', 'a', 'toss'],\n",
    "    ['So', 'lets', 'understand', 'the', 'core', 'philosophy', 'and', 'then', 'move', 'on', 'to', 'profitability,', 'EBITDA,', 'and', 'other', 'numbers'],\n",
    "    ['Now', 'the', 'question', 'is,', 'what', 'exactly', 'is', 'the', 'secret', 'recipe', 'for', 'Hyundai', 'success', 'in', 'India?'],\n",
    "    ['This', 'story', 'dates', 'back', 'to', 'the', 'early', '1990s'],\n",
    "    ['Until', 'this', 'point,', 'India', 'only', 'had', 'a', 'few', 'car', 'brands:', 'Hindustan', 'Ambassador,', 'Contessa,', 'Premier', 'Padmini,', 'Standard', 'Herald,', '2000,', 'Maruti', '800,', 'Omni,', 'and', 'Gypsy']\n",
    "]\n",
    "\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' la = []\\n\\nstart = 0\\nfor i in te:\\n    #print(i)\\n    end = len(i)\\n    print(start,(start+end))\\n    la += [labels[start:(start+end)]]\\n    start = end\\n\\nprint(la)\\n\\nprint(len(la)) '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' la = []\n",
    "\n",
    "start = 0\n",
    "for i in te:\n",
    "    #print(i)\n",
    "    end = len(i)\n",
    "    print(start,(start+end))\n",
    "    la += [labels[start:(start+end)]]\n",
    "    start = end\n",
    "\n",
    "print(la)\n",
    "\n",
    "print(len(la)) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for i in te:\\n    print(i)\\n    print(len(i)) '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' for i in te:\n",
    "    print(i)\n",
    "    print(len(i)) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for i in la:\\n    print(i) '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' for i in la:\n",
    "    print(i) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "lab = [['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC'],\n",
    "['B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O'],\n",
    "['O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-ORG', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC'],\n",
    "['O', 'O', 'B-MISC', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" 'Hyundai': 'B-ORG',\\n    'India': 'B-LOC',\\n    'South Korean': 'B-LOC',\\n    'IPO': 'B-MISC',\\n    'Maruti': 'B-ORG',\\n    'SEBI': 'B-ORG',\\n    'Scaler School of Business': 'B-ORG' \""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' 'Hyundai': 'B-ORG',\n",
    "    'India': 'B-LOC',\n",
    "    'South Korean': 'B-LOC',\n",
    "    'IPO': 'B-MISC',\n",
    "    'Maruti': 'B-ORG',\n",
    "    'SEBI': 'B-ORG',\n",
    "    'Scaler School of Business': 'B-ORG' '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for i in range(len(te)):\n",
    "    for j in range(len(te[i])):\n",
    "        if te[i][j] == 'Hyundai':\n",
    "            lab[i][j] = 'B-ORG'\n",
    "\n",
    "        elif te[i][j] == 'India':\n",
    "            lab[i][j] = 'B-LOC'\n",
    "\n",
    "        elif te[i][j] == 'South':\n",
    "            lab[i][j] = 'B-LOC'\n",
    "\n",
    "        elif te[i][j] == 'Korean':\n",
    "            lab[i][j] = 'B-LOC'\n",
    "\n",
    "        elif te[i][j] == 'IPO':\n",
    "            lab[i][j] = 'B-MISC'\n",
    "\n",
    "        elif te[i][j] == 'Maruti':\n",
    "            lab[i][j] = 'B-ORG'\n",
    "\n",
    "        elif te[i][j] == 'Mahindra':\n",
    "            lab[i][j] = 'B-ORG'\n",
    "\n",
    "        elif te[i][j] == 'Tata':\n",
    "            lab[i][j] = 'B-ORG'\n",
    "\n",
    "        elif te[i][j] == 'SEBI':\n",
    "            lab[i][j] = 'B-ORG'\n",
    "\n",
    "        elif te[i][j] == 'School':\n",
    "            lab[i][j] = 'B-ORG'\n",
    "\n",
    "        else:\n",
    "            lab[i][j] = 'O'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC']\n",
      "['B-LOC', 'B-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n",
      "['O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-LOC']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n",
      "['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'B-ORG']\n",
      "['O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC']\n",
      "['O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'B-ORG']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-LOC']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "for i in lab:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\ndatase = Dataset.from_dict({\\n    \"id\": [\\'0\\', \\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\', \\'7\\', \\'8\\', \\'9\\', \\'10\\', \\'11\\', \\'12\\', \\'13\\', \\'14\\', \\'15\\', \\'16\\', \\'17\\', \\'18\\', \\'19\\', \\'20\\', \\'21\\', \\'22\\', \\'23\\', \\'24\\', \\'25\\', \\'26\\', \\'27\\', \\'28\\', \\'29\\', \\'30\\', \\'31\\', \\'32\\', \\'33\\', \\'34\\', \\'35\\', \\'36\\', \\'37\\', \\'38\\'],\\n    \"text\": [\\n    [\\'Hi\\', \\'everybody!\\', \\'Hyundai\\', \\'is\\', \\'one\\', \\'of\\', \\'the\\', \\'most\\', \\'iconic\\', \\'companies\\', \\'to\\', \\'enter\\', \\'the\\', \\'Indian\\', \\'markets\\'],\\n    [\\'It\\', \\'is\\', \\'hard\\', \\'to\\', \\'believe\\', \\'that\\', \\'a\\', \\'South\\', \\'Korean\\', \\'brand,\\', \\'which\\', \\'was\\', \\'once\\', \\'a\\', \\'difficult\\', \\'name\\', \\'to\\', \\'pronounce,\\', \\'is\\', \\'today\\', \\'about\\', \\'to\\', \\'launch\\', \\'the\\', \\'biggest\\', \\'IPO\\', \\'in\\', \\'the\\', \\'history\\', \\'of\\', \\'India\\'],\\n    [\\'Indian\\', \\'stock\\', \\'markets\\', \\'could\\', \\'soon\\', \\'see\\', \\'their\\', \\'biggest\\', \\'ever\\', \\'IPO\\'],\\n    [\\'South\\', \\'Korean\\', \\'automaker\\', \\'Hyundai\\', \\'is\\', \\'gearing\\', \\'up\\', \\'for\\', \\'a\\', \\'significant\\', \\'milestone\\', \\'by\\', \\'going\\', \\'public\\', \\'in\\', \\'India\\'],\\n    [\\'Hyundais\\', \\'IPO\\', \\'is\\', \\'poised\\', \\'to\\', \\'be\\', \\'one\\', \\'of\\', \\'the\\', \\'largest\\', \\'in\\', \\'recent\\', \\'times,\\', \\'potentially\\', \\'the\\', \\'biggest\\', \\'IPO\\', \\'ever\\', \\'in\\', \\'India\\'],\\n    [\\'This\\', \\'would\\', \\'be\\', \\'Indias\\', \\'largest\\', \\'IPO\\', \\'and\\', \\'the\\', \\'first\\', \\'van\\', \\'automaker\\', \\'since\\', \\'Maruti\\', \\'Suzuki\\', \\'in\\', \\'2003\\'],\\n    [\\'In\\', \\'1996,\\', \\'when\\', \\'Hyundai\\', \\'entered\\', \\'the\\', \\'Indian\\', \\'market,\\', \\'Maruti\\', \\'was\\', \\'the\\', \\'most\\', \\'dominant\\', \\'player\\', \\'with\\', \\'a\\', \\'60%\\', \\'market\\', \\'share,\\', \\'almost\\', \\'a\\', \\'monopoly\\', \\'amongst\\', \\'the\\', \\'middle\\', \\'class\\', \\'in\\', \\'India\\'],\\n    [\\'Despite\\', \\'this,\\', \\'Hyundai\\', \\'came\\', \\'in\\', \\'with\\', \\'models\\', \\'like\\', \\'Santro,\\', \\'i10,\\', \\'i20,\\', \\'and\\', \\'Creta,\\', \\'making\\', \\'a\\', \\'significant\\', \\'impact\\'],\\n    [\\'Today,\\', \\'Hyundai\\', \\'is\\', \\'the\\', \\'second-largest\\', \\'automaker\\', \\'in\\', \\'the\\', \\'country\\', \\'with\\', \\'a\\', \\'profit\\', \\'of\\', \\'4,653\\', \\'crores\\'],\\n    [\\'In\\', \\'terms\\', \\'of\\', \\'profitability,\\', \\'Hyundai\\', \\'is\\', \\'way\\', \\'ahead\\', \\'of\\', \\'Maruti\\'],\\n    [\\'In\\', \\'FY23,\\', \\'while\\', \\'Hyundai\\', \\'made\\', \\'a\\', \\'profit\\', \\'of\\', \\'65,355\\', \\'rupees\\', \\'per\\', \\'vehicle,\\', \\'Maruti\\', \\'made\\', \\'a\\', \\'profit\\', \\'of\\', \\'4,939\\', \\'rupees\\', \\'per\\', \\'vehicle\\'],\\n    [\\'Now,\\', \\'after\\', \\'28\\', \\'years\\', \\'of\\', \\'establishing\\', \\'a\\', \\'stronghold\\', \\'in\\', \\'the\\', \\'Indian\\', \\'market,\\', \\'Hyundai\\', \\'plans\\', \\'to\\', \\'raise\\', \\'25,000\\', \\'crores\\', \\'with\\', \\'its\\', \\'IPO\\'],\\n    [\\'Sounds\\', \\'fantastic,\\', \\'right?\\'],\\n    [\\'However,\\', \\'there\\', \\'are\\', \\'some\\', \\'hidden\\', \\'risks\\', \\'that\\', \\'Hyundai\\', \\'faces\\', \\'which\\', \\'may\\', \\'jeopardize\\', \\'all\\', \\'the\\', \\'progress\\', \\'theyve\\', \\'made\\', \\'in\\', \\'India\\'],\\n    [\\'All\\', \\'of\\', \\'this\\', \\'is\\', \\'detailed\\', \\'in\\', \\'a\\', \\'436-page\\', \\'DRHP\\', \\'paper\\', \\'that\\', \\'Hyundai\\', \\'filed\\', \\'with\\', \\'SEBI\\'],\\n    [\\'Weve\\', \\'gone\\', \\'through\\', \\'all\\', \\'this\\', \\'data,\\', \\'so\\', \\'that\\', \\'you\\', \\'can\\', \\'sit\\', \\'back\\', \\'and\\', \\'consume\\', \\'the\\', \\'most\\', \\'important\\', \\'bits\\', \\'of\\', \\'this\\', \\'400-page\\', \\'document\\', \\'as\\', \\'easily\\', \\'as\\', \\'watching\\', \\'a\\', \\'movie\\'],\\n    [\\'In\\', \\'this\\', \\'episode,\\', \\'lets\\', \\'dive\\', \\'into\\', \\'understanding\\', \\'how\\', \\'a\\', \\'South\\', \\'Korean\\', \\'company\\', \\'like\\', \\'Hyundai\\', \\'broke\\', \\'into\\', \\'the\\', \\'Indian\\', \\'automobile\\', \\'market,\\', \\'leaving\\', \\'behind\\', \\'giants\\', \\'like\\', \\'Tata\\', \\'and\\', \\'Mahindra\\'],\\n    [\\'Despite\\', \\'being\\', \\'a\\', \\'foreign\\', \\'company,\\', \\'how\\', \\'did\\', \\'Hyundai\\', \\'achieve\\', \\'such\\', \\'high\\', \\'profitability\\', \\'and\\', \\'outpace\\', \\'a\\', \\'mammoth\\', \\'like\\', \\'Maruti\\', \\'in\\', \\'India?\\'],\\n    [\\'Most\\', \\'importantly,\\', \\'after\\', \\'making\\', \\'such\\', \\'amazing\\', \\'progress,\\', \\'what\\', \\'are\\', \\'the\\', \\'hidden\\', \\'risks\\', \\'that\\', \\'could\\', \\'topple\\', \\'Hyundai\\', \\'growth\\', \\'in\\', \\'India?\\'],\\n    [\\'Also,\\', \\'a\\', \\'quick\\', \\'disclaimer:\\', \\'I\\', \\'am\\', \\'not\\', \\'a\\', \\'SEBI\\',\\'registered\\', \\'investment\\', \\'adviser,\\', \\'and\\', \\'this\\', \\'video\\', \\'is\\', \\'not\\', \\'meant\\', \\'to\\', \\'give\\', \\'you\\', \\'investment\\', \\'advice\\', \\'but\\', \\'to\\', \\'only\\', \\'educate\\', \\'you\\', \\'about\\', \\'the\\', \\'rise\\', \\'of\\', \\'Hyundai\\', \\'in\\', \\'India\\'],\\n    [\\'Before\\', \\'we\\', \\'move\\', \\'on,\\', \\'I\\', \\'want\\', \\'to\\', \\'quickly\\', \\'introduce\\', \\'you\\', \\'to\\', \\'our\\', \\'education\\', \\'partners\\', \\'of\\', \\'todays\\', \\'episode,\\', \\'Scaler\\', \\'School\\', \\'of\\', \\'Business\\'],\\n    [\\'They\\', \\'are\\', \\'bringing\\', \\'something\\', \\'absolutely\\', \\'revolutionary\\'],\\n    [\\'If\\', \\'youre\\', \\'seeking\\', \\'business\\', \\'education,\\', \\'Scaler\\', \\'is\\', \\'offering\\', \\'a\\', \\'full-time\\', \\'on-campus\\', \\'18-month\\', \\'PG\\', \\'program\\', \\'in\\', \\'management\\', \\'and\\', \\'technology\\'],\\n    [\\'This\\', \\'program\\', \\'is\\', \\'designed\\', \\'by\\', \\'leaders\\', \\'who\\', \\'have\\', \\'built\\', \\'billion-dollar\\', \\'businesses\\', \\'like\\', \\'Uber,\\', \\'Myntra,\\', \\'and\\', \\'Meesho\\'],\\n    [\\'Today,\\', \\'legacy\\', \\'college\\', \\'names\\', \\'are\\', \\'useless\\', \\'without\\', \\'competitive\\', \\'skill\\', \\'sets\\'],\\n    [\\'This\\', \\'is\\', \\'why,\\', \\'as\\', \\'part\\', \\'of\\', \\'the\\', \\'Scaler\\', \\'School\\', \\'of\\', \\'Business\\', \\'curriculum,\\', \\'you\\', \\'will\\', \\'work\\', \\'on\\', \\'real-world\\', \\'projects\\', \\'sourced\\', \\'from\\', \\'real\\', \\'companies\\'],\\n    [\\'You\\', \\'will\\', \\'build\\', \\'your\\', \\'own\\', \\'business,\\', \\'take\\', \\'it\\', \\'to\\', \\'the\\', \\'market,\\', \\'generate\\', \\'revenue,\\', \\'and\\', \\'raise\\', \\'VC\\', \\'capital\\', \\'while\\', \\'studying\\', \\'on\\', \\'campus\\'],\\n    [\\'Since\\', \\'Scaler\\', \\'has\\', \\'been\\', \\'in\\', \\'the\\', \\'education\\', \\'industry\\', \\'for\\', \\'seven\\', \\'long\\', \\'years,\\', \\'they\\', \\'already\\', \\'have\\', \\'access\\', \\'to\\', \\'1,200+\\', \\'company\\', \\'partners,\\', \\'something\\', \\'no\\', \\'other\\', \\'B-School\\', \\'in\\', \\'the\\', \\'country\\', \\'can\\', \\'offer\\', \\'right\\', \\'now\\'],\\n    [\\'For\\', \\'their\\', \\'online\\', \\'programs,\\', \\'theyve\\', \\'seen\\', \\'placement\\', \\'rates\\', \\'of\\', \\'96%\\', \\'with\\', \\'a\\', \\'median\\', \\'CTC\\', \\'of\\', \\'25\\', \\'lakh\\', \\'rupees,\\', \\'verified\\', \\'by\\', \\'B2K\\', \\'Analytics,\\', \\'the\\', \\'same\\', \\'partner\\', \\'that\\', \\'verifies\\', \\'reports\\', \\'for\\', \\'IIMs\\'],\\n    [\\'Now,\\', \\'Scaler\\', \\'School\\', \\'of\\', \\'Business\\', \\'is\\', \\'handpicking\\', \\'only\\', \\'75\\', \\'students\\', \\'for\\', \\'their\\', \\'founding\\', \\'cohort\\', \\'starting\\', \\'in\\', \\'August\\', \\'2024\\'],\\n    [\\'They\\', \\'have\\', \\'scholarships\\', \\'up\\', \\'to\\', \\'100%,\\', \\'and\\', \\'applications\\', \\'are\\', \\'open\\', \\'right\\', \\'now\\', \\'If\\', \\'you\\', \\'want\\', \\'to\\', \\'become\\', \\'a\\', \\'brilliant\\', \\'business\\', \\'leader,\\', \\'apply\\', \\'for\\', \\'Scaler\\', \\'School\\', \\'of\\', \\'Business\\', \\'using\\', \\'the\\', \\'link\\', \\'in\\', \\'my\\', \\'description\\', \\'and\\', \\'in\\', \\'the\\', \\'comment\\', \\'section\\'],\\n    [\\'Now,\\', \\'on\\', \\'with\\', \\'the\\', \\'episode\\'],\\n    [\\'The\\', \\'first\\', \\'thing\\', \\'you\\', \\'need\\', \\'to\\', \\'understand\\', \\'about\\', \\'a\\', \\'company\\', \\'is\\', \\'their\\', \\'philosophy\\', \\'of\\', \\'success\\'],\\n    [\\'In\\', \\'this\\', \\'case,\\', \\'you\\', \\'need\\', \\'to\\', \\'understand\\', \\'what\\', \\'exactly\\', \\'is\\', \\'the\\', \\'philosophy\\', \\'that\\', \\'turned\\', \\'Hyundai\\', \\'into\\', \\'what\\', \\'it\\', \\'is\\', \\'today\\', \\'in\\', \\'India\\'],\\n    [\\'Like\\', \\'we\\', \\'saw\\', \\'in\\', \\'the\\', \\'Boeing\\', \\'video\\', \\'no\\', \\'matter\\', \\'how\\', \\'much\\', \\'profitability,\\', \\'EBITDA,\\', \\'or\\', \\'earnings\\', \\'per\\', \\'share\\', \\'a\\', \\'company\\', \\'has\\', \\'if\\', \\'it\\', \\'deviates\\', \\'from\\', \\'its\\', \\'core\\', \\'philosophy\\', \\'all\\', \\'these\\', \\'numbers\\', \\'will\\', \\'go\\', \\'for\\', \\'a\\', \\'toss\\'],\\n    [\\'So\\', \\'lets\\', \\'understand\\', \\'the\\', \\'core\\', \\'philosophy\\', \\'and\\', \\'then\\', \\'move\\', \\'on\\', \\'to\\', \\'profitability,\\', \\'EBITDA,\\', \\'and\\', \\'other\\', \\'numbers\\'],\\n    [\\'Now\\', \\'the\\', \\'question\\', \\'is,\\', \\'what\\', \\'exactly\\', \\'is\\', \\'the\\', \\'secret\\', \\'recipe\\', \\'for\\', \\'Hyundai\\', \\'success\\', \\'in\\', \\'India?\\'],\\n    [\\'This\\', \\'story\\', \\'dates\\', \\'back\\', \\'to\\', \\'the\\', \\'early\\', \\'1990s\\'],\\n    [\\'Until\\', \\'this\\', \\'point,\\', \\'India\\', \\'only\\', \\'had\\', \\'a\\', \\'few\\', \\'car\\', \\'brands:\\', \\'Hindustan\\', \\'Ambassador,\\', \\'Contessa,\\', \\'Premier\\', \\'Padmini,\\', \\'Standard\\', \\'Herald,\\', \\'2000,\\', \\'Maruti\\', \\'800,\\', \\'Omni,\\', \\'and\\', \\'Gypsy\\']\\n],\\n    \"labels\" : [[\\'O\\', \\'O\\', \\'B-ORG\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'B-LOC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\'],\\n[\\'B-LOC\\', \\'B-LOC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'B-LOC\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'B-LOC\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\',\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'B-ORG\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\'],\\n[\\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\'],\\n[\\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\']]\\n})\\n\\n '"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "datase = Dataset.from_dict({\n",
    "    \"id\": ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38'],\n",
    "    \"text\": [\n",
    "    ['Hi', 'everybody!', 'Hyundai', 'is', 'one', 'of', 'the', 'most', 'iconic', 'companies', 'to', 'enter', 'the', 'Indian', 'markets'],\n",
    "    ['It', 'is', 'hard', 'to', 'believe', 'that', 'a', 'South', 'Korean', 'brand,', 'which', 'was', 'once', 'a', 'difficult', 'name', 'to', 'pronounce,', 'is', 'today', 'about', 'to', 'launch', 'the', 'biggest', 'IPO', 'in', 'the', 'history', 'of', 'India'],\n",
    "    ['Indian', 'stock', 'markets', 'could', 'soon', 'see', 'their', 'biggest', 'ever', 'IPO'],\n",
    "    ['South', 'Korean', 'automaker', 'Hyundai', 'is', 'gearing', 'up', 'for', 'a', 'significant', 'milestone', 'by', 'going', 'public', 'in', 'India'],\n",
    "    ['Hyundais', 'IPO', 'is', 'poised', 'to', 'be', 'one', 'of', 'the', 'largest', 'in', 'recent', 'times,', 'potentially', 'the', 'biggest', 'IPO', 'ever', 'in', 'India'],\n",
    "    ['This', 'would', 'be', 'Indias', 'largest', 'IPO', 'and', 'the', 'first', 'van', 'automaker', 'since', 'Maruti', 'Suzuki', 'in', '2003'],\n",
    "    ['In', '1996,', 'when', 'Hyundai', 'entered', 'the', 'Indian', 'market,', 'Maruti', 'was', 'the', 'most', 'dominant', 'player', 'with', 'a', '60%', 'market', 'share,', 'almost', 'a', 'monopoly', 'amongst', 'the', 'middle', 'class', 'in', 'India'],\n",
    "    ['Despite', 'this,', 'Hyundai', 'came', 'in', 'with', 'models', 'like', 'Santro,', 'i10,', 'i20,', 'and', 'Creta,', 'making', 'a', 'significant', 'impact'],\n",
    "    ['Today,', 'Hyundai', 'is', 'the', 'second-largest', 'automaker', 'in', 'the', 'country', 'with', 'a', 'profit', 'of', '4,653', 'crores'],\n",
    "    ['In', 'terms', 'of', 'profitability,', 'Hyundai', 'is', 'way', 'ahead', 'of', 'Maruti'],\n",
    "    ['In', 'FY23,', 'while', 'Hyundai', 'made', 'a', 'profit', 'of', '65,355', 'rupees', 'per', 'vehicle,', 'Maruti', 'made', 'a', 'profit', 'of', '4,939', 'rupees', 'per', 'vehicle'],\n",
    "    ['Now,', 'after', '28', 'years', 'of', 'establishing', 'a', 'stronghold', 'in', 'the', 'Indian', 'market,', 'Hyundai', 'plans', 'to', 'raise', '25,000', 'crores', 'with', 'its', 'IPO'],\n",
    "    ['Sounds', 'fantastic,', 'right?'],\n",
    "    ['However,', 'there', 'are', 'some', 'hidden', 'risks', 'that', 'Hyundai', 'faces', 'which', 'may', 'jeopardize', 'all', 'the', 'progress', 'theyve', 'made', 'in', 'India'],\n",
    "    ['All', 'of', 'this', 'is', 'detailed', 'in', 'a', '436-page', 'DRHP', 'paper', 'that', 'Hyundai', 'filed', 'with', 'SEBI'],\n",
    "    ['Weve', 'gone', 'through', 'all', 'this', 'data,', 'so', 'that', 'you', 'can', 'sit', 'back', 'and', 'consume', 'the', 'most', 'important', 'bits', 'of', 'this', '400-page', 'document', 'as', 'easily', 'as', 'watching', 'a', 'movie'],\n",
    "    ['In', 'this', 'episode,', 'lets', 'dive', 'into', 'understanding', 'how', 'a', 'South', 'Korean', 'company', 'like', 'Hyundai', 'broke', 'into', 'the', 'Indian', 'automobile', 'market,', 'leaving', 'behind', 'giants', 'like', 'Tata', 'and', 'Mahindra'],\n",
    "    ['Despite', 'being', 'a', 'foreign', 'company,', 'how', 'did', 'Hyundai', 'achieve', 'such', 'high', 'profitability', 'and', 'outpace', 'a', 'mammoth', 'like', 'Maruti', 'in', 'India?'],\n",
    "    ['Most', 'importantly,', 'after', 'making', 'such', 'amazing', 'progress,', 'what', 'are', 'the', 'hidden', 'risks', 'that', 'could', 'topple', 'Hyundai', 'growth', 'in', 'India?'],\n",
    "    ['Also,', 'a', 'quick', 'disclaimer:', 'I', 'am', 'not', 'a', 'SEBI','registered', 'investment', 'adviser,', 'and', 'this', 'video', 'is', 'not', 'meant', 'to', 'give', 'you', 'investment', 'advice', 'but', 'to', 'only', 'educate', 'you', 'about', 'the', 'rise', 'of', 'Hyundai', 'in', 'India'],\n",
    "    ['Before', 'we', 'move', 'on,', 'I', 'want', 'to', 'quickly', 'introduce', 'you', 'to', 'our', 'education', 'partners', 'of', 'todays', 'episode,', 'Scaler', 'School', 'of', 'Business'],\n",
    "    ['They', 'are', 'bringing', 'something', 'absolutely', 'revolutionary'],\n",
    "    ['If', 'youre', 'seeking', 'business', 'education,', 'Scaler', 'is', 'offering', 'a', 'full-time', 'on-campus', '18-month', 'PG', 'program', 'in', 'management', 'and', 'technology'],\n",
    "    ['This', 'program', 'is', 'designed', 'by', 'leaders', 'who', 'have', 'built', 'billion-dollar', 'businesses', 'like', 'Uber,', 'Myntra,', 'and', 'Meesho'],\n",
    "    ['Today,', 'legacy', 'college', 'names', 'are', 'useless', 'without', 'competitive', 'skill', 'sets'],\n",
    "    ['This', 'is', 'why,', 'as', 'part', 'of', 'the', 'Scaler', 'School', 'of', 'Business', 'curriculum,', 'you', 'will', 'work', 'on', 'real-world', 'projects', 'sourced', 'from', 'real', 'companies'],\n",
    "    ['You', 'will', 'build', 'your', 'own', 'business,', 'take', 'it', 'to', 'the', 'market,', 'generate', 'revenue,', 'and', 'raise', 'VC', 'capital', 'while', 'studying', 'on', 'campus'],\n",
    "    ['Since', 'Scaler', 'has', 'been', 'in', 'the', 'education', 'industry', 'for', 'seven', 'long', 'years,', 'they', 'already', 'have', 'access', 'to', '1,200+', 'company', 'partners,', 'something', 'no', 'other', 'B-School', 'in', 'the', 'country', 'can', 'offer', 'right', 'now'],\n",
    "    ['For', 'their', 'online', 'programs,', 'theyve', 'seen', 'placement', 'rates', 'of', '96%', 'with', 'a', 'median', 'CTC', 'of', '25', 'lakh', 'rupees,', 'verified', 'by', 'B2K', 'Analytics,', 'the', 'same', 'partner', 'that', 'verifies', 'reports', 'for', 'IIMs'],\n",
    "    ['Now,', 'Scaler', 'School', 'of', 'Business', 'is', 'handpicking', 'only', '75', 'students', 'for', 'their', 'founding', 'cohort', 'starting', 'in', 'August', '2024'],\n",
    "    ['They', 'have', 'scholarships', 'up', 'to', '100%,', 'and', 'applications', 'are', 'open', 'right', 'now', 'If', 'you', 'want', 'to', 'become', 'a', 'brilliant', 'business', 'leader,', 'apply', 'for', 'Scaler', 'School', 'of', 'Business', 'using', 'the', 'link', 'in', 'my', 'description', 'and', 'in', 'the', 'comment', 'section'],\n",
    "    ['Now,', 'on', 'with', 'the', 'episode'],\n",
    "    ['The', 'first', 'thing', 'you', 'need', 'to', 'understand', 'about', 'a', 'company', 'is', 'their', 'philosophy', 'of', 'success'],\n",
    "    ['In', 'this', 'case,', 'you', 'need', 'to', 'understand', 'what', 'exactly', 'is', 'the', 'philosophy', 'that', 'turned', 'Hyundai', 'into', 'what', 'it', 'is', 'today', 'in', 'India'],\n",
    "    ['Like', 'we', 'saw', 'in', 'the', 'Boeing', 'video', 'no', 'matter', 'how', 'much', 'profitability,', 'EBITDA,', 'or', 'earnings', 'per', 'share', 'a', 'company', 'has', 'if', 'it', 'deviates', 'from', 'its', 'core', 'philosophy', 'all', 'these', 'numbers', 'will', 'go', 'for', 'a', 'toss'],\n",
    "    ['So', 'lets', 'understand', 'the', 'core', 'philosophy', 'and', 'then', 'move', 'on', 'to', 'profitability,', 'EBITDA,', 'and', 'other', 'numbers'],\n",
    "    ['Now', 'the', 'question', 'is,', 'what', 'exactly', 'is', 'the', 'secret', 'recipe', 'for', 'Hyundai', 'success', 'in', 'India?'],\n",
    "    ['This', 'story', 'dates', 'back', 'to', 'the', 'early', '1990s'],\n",
    "    ['Until', 'this', 'point,', 'India', 'only', 'had', 'a', 'few', 'car', 'brands:', 'Hindustan', 'Ambassador,', 'Contessa,', 'Premier', 'Padmini,', 'Standard', 'Herald,', '2000,', 'Maruti', '800,', 'Omni,', 'and', 'Gypsy']\n",
    "],\n",
    "    \"labels\" : [['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC'],\n",
    "['B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O'],\n",
    "['O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC','O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-ORG', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC'],\n",
    "['O', 'O', 'B-MISC', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
    "})\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(datase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                               text  \\\n",
      "0    0  [Hi, everybody!, Hyundai, is, one, of, the, mo...   \n",
      "1    1  [It, is, hard, to, believe, that, a, South, Ko...   \n",
      "2    2  [Indian, stock, markets, could, soon, see, the...   \n",
      "3    3  [South, Korean, automaker, Hyundai, is, gearin...   \n",
      "4    4  [Hyundais, IPO, is, poised, to, be, one, of, t...   \n",
      "5    5  [This, would, be, Indias, largest, IPO, and, t...   \n",
      "6    6  [In, 1996,, when, Hyundai, entered, the, India...   \n",
      "7    7  [Despite, this,, Hyundai, came, in, with, mode...   \n",
      "8    8  [Today,, Hyundai, is, the, second-largest, aut...   \n",
      "9    9  [In, terms, of, profitability,, Hyundai, is, w...   \n",
      "10  10  [In, FY23,, while, Hyundai, made, a, profit, o...   \n",
      "11  11  [Now,, after, 28, years, of, establishing, a, ...   \n",
      "12  12                       [Sounds, fantastic,, right?]   \n",
      "13  13  [However,, there, are, some, hidden, risks, th...   \n",
      "14  14  [All, of, this, is, detailed, in, a, 436-page,...   \n",
      "15  15  [Weve, gone, through, all, this, data,, so, th...   \n",
      "16  16  [In, this, episode,, lets, dive, into, underst...   \n",
      "17  17  [Despite, being, a, foreign, company,, how, di...   \n",
      "18  18  [Most, importantly,, after, making, such, amaz...   \n",
      "19  19  [Also,, a, quick, disclaimer:, I, am, not, a, ...   \n",
      "20  20  [Before, we, move, on,, I, want, to, quickly, ...   \n",
      "21  21  [They, are, bringing, something, absolutely, r...   \n",
      "22  22  [If, youre, seeking, business, education,, Sca...   \n",
      "23  23  [This, program, is, designed, by, leaders, who...   \n",
      "24  24  [Today,, legacy, college, names, are, useless,...   \n",
      "25  25  [This, is, why,, as, part, of, the, Scaler, Sc...   \n",
      "26  26  [You, will, build, your, own, business,, take,...   \n",
      "27  27  [Since, Scaler, has, been, in, the, education,...   \n",
      "28  28  [For, their, online, programs,, theyve, seen, ...   \n",
      "29  29  [Now,, Scaler, School, of, Business, is, handp...   \n",
      "30  30  [They, have, scholarships, up, to, 100%,, and,...   \n",
      "31  31                     [Now,, on, with, the, episode]   \n",
      "32  32  [The, first, thing, you, need, to, understand,...   \n",
      "33  33  [In, this, case,, you, need, to, understand, w...   \n",
      "34  34  [Like, we, saw, in, the, Boeing, video, no, ma...   \n",
      "35  35  [So, lets, understand, the, core, philosophy, ...   \n",
      "36  36  [Now, the, question, is,, what, exactly, is, t...   \n",
      "37  37  [This, story, dates, back, to, the, early, 1990s]   \n",
      "38  38  [Until, this, point,, India, only, had, a, few...   \n",
      "\n",
      "                                               labels  \n",
      "0   [O, O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "1   [O, O, O, O, O, O, O, B-LOC, B-LOC, O, O, O, O...  \n",
      "2                 [O, O, O, O, O, O, O, O, O, B-MISC]  \n",
      "3   [B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O...  \n",
      "4   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "5   [O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O,...  \n",
      "6   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "7   [O, O, O, O, O, O, O, O, O, O, O, O, B-MISC, O...  \n",
      "8       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "9                      [O, O, O, O, O, O, O, O, O, O]  \n",
      "10  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "11  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "12                                          [O, O, O]  \n",
      "13  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "14      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "15  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "16  [O, O, O, O, O, O, O, O, O, B-LOC, B-LOC, O, B...  \n",
      "17  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-MISC...  \n",
      "18  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "19  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "20  [O, O, O, O, O, O, B-MISC, O, O, O, O, B-LOC, ...  \n",
      "21                                 [O, O, O, O, O, O]  \n",
      "22  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "23   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "24                     [O, O, O, O, O, O, O, O, O, O]  \n",
      "25  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "26  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "27  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "28  [O, O, O, O, O, O, O, O, O, B-MISC, O, O, O, O...  \n",
      "29  [O, O, O, O, O, O, O, O, O, O, B-MISC, O, O, O...  \n",
      "30  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "31                               [O, O, B-MISC, O, O]  \n",
      "32      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "33  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "34  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "35  [O, O, O, O, O, B-MISC, O, O, O, O, B-LOC, O, ...  \n",
      "36      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "37                           [O, O, O, O, O, O, O, O]  \n",
      "38  [O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O,...  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Trained Model Hugging Face (Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "datase = Dataset.from_dict({\n",
    "    \"id\": ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38'],\n",
    "    \"text\": [\n",
    "        ['Hi', 'everybody!', 'Hyundai', 'is', 'one', 'of', 'the', 'most', 'iconic', 'companies', 'to', 'enter', 'the', 'Indian', 'markets'],\n",
    "    ['It', 'is', 'hard', 'to', 'believe', 'that', 'a', 'South', 'Korean', 'brand,', 'which', 'was', 'once', 'a', 'difficult', 'name', 'to', 'pronounce,', 'is', 'today', 'about', 'to', 'launch', 'the', 'biggest', 'IPO', 'in', 'the', 'history', 'of', 'India'],\n",
    "    ['Indian', 'stock', 'markets', 'could', 'soon', 'see', 'their', 'biggest', 'ever', 'IPO'],\n",
    "    ['South', 'Korean', 'automaker', 'Hyundai', 'is', 'gearing', 'up', 'for', 'a', 'significant', 'milestone', 'by', 'going', 'public', 'in', 'India'],\n",
    "    ['Hyundais', 'IPO', 'is', 'poised', 'to', 'be', 'one', 'of', 'the', 'largest', 'in', 'recent', 'times,', 'potentially', 'the', 'biggest', 'IPO', 'ever', 'in', 'India'],\n",
    "    ['This', 'would', 'be', 'Indias', 'largest', 'IPO', 'and', 'the', 'first', 'van', 'automaker', 'since', 'Maruti', 'Suzuki', 'in', '2003'],\n",
    "    ['In', '1996,', 'when', 'Hyundai', 'entered', 'the', 'Indian', 'market,', 'Maruti', 'was', 'the', 'most', 'dominant', 'player', 'with', 'a', '60%', 'market', 'share,', 'almost', 'a', 'monopoly', 'amongst', 'the', 'middle', 'class', 'in', 'India'],\n",
    "    ['Despite', 'this,', 'Hyundai', 'came', 'in', 'with', 'models', 'like', 'Santro,', 'i10,', 'i20,', 'and', 'Creta,', 'making', 'a', 'significant', 'impact'],\n",
    "    ['Today,', 'Hyundai', 'is', 'the', 'second-largest', 'automaker', 'in', 'the', 'country', 'with', 'a', 'profit', 'of', '4,653', 'crores'],\n",
    "    ['In', 'terms', 'of', 'profitability,', 'Hyundai', 'is', 'way', 'ahead', 'of', 'Maruti'],\n",
    "    ['In', 'FY23,', 'while', 'Hyundai', 'made', 'a', 'profit', 'of', '65,355', 'rupees', 'per', 'vehicle,', 'Maruti', 'made', 'a', 'profit', 'of', '4,939', 'rupees', 'per', 'vehicle'],\n",
    "    ['Now,', 'after', '28', 'years', 'of', 'establishing', 'a', 'stronghold', 'in', 'the', 'Indian', 'market,', 'Hyundai', 'plans', 'to', 'raise', '25,000', 'crores', 'with', 'its', 'IPO'],\n",
    "    ['Sounds', 'fantastic,', 'right?'],\n",
    "    ['However,', 'there', 'are', 'some', 'hidden', 'risks', 'that', 'Hyundai', 'faces', 'which', 'may', 'jeopardize', 'all', 'the', 'progress', 'theyve', 'made', 'in', 'India'],\n",
    "    ['All', 'of', 'this', 'is', 'detailed', 'in', 'a', '436-page', 'DRHP', 'paper', 'that', 'Hyundai', 'filed', 'with', 'SEBI'],\n",
    "    ['Weve', 'gone', 'through', 'all', 'this', 'data,', 'so', 'that', 'you', 'can', 'sit', 'back', 'and', 'consume', 'the', 'most', 'important', 'bits', 'of', 'this', '400-page', 'document', 'as', 'easily', 'as', 'watching', 'a', 'movie'],\n",
    "    ['In', 'this', 'episode,', 'lets', 'dive', 'into', 'understanding', 'how', 'a', 'South', 'Korean', 'company', 'like', 'Hyundai', 'broke', 'into', 'the', 'Indian', 'automobile', 'market,', 'leaving', 'behind', 'giants', 'like', 'Tata', 'and', 'Mahindra'],\n",
    "    ['Despite', 'being', 'a', 'foreign', 'company,', 'how', 'did', 'Hyundai', 'achieve', 'such', 'high', 'profitability', 'and', 'outpace', 'a', 'mammoth', 'like', 'Maruti', 'in', 'India?'],\n",
    "    ['Most', 'importantly,', 'after', 'making', 'such', 'amazing', 'progress,', 'what', 'are', 'the', 'hidden', 'risks', 'that', 'could', 'topple', 'Hyundai', 'growth', 'in', 'India?'],\n",
    "    ['Also,', 'a', 'quick', 'disclaimer:', 'I', 'am', 'not', 'a', 'SEBI', 'registered', 'investment', 'adviser,', 'and', 'this', 'video', 'is', 'not', 'meant', 'to', 'give', 'you', 'investment', 'advice', 'but', 'to', 'only', 'educate', 'you', 'about', 'the', 'rise', 'of', 'Hyundai', 'in', 'India'],\n",
    "    ['Before', 'we', 'move', 'on,', 'I', 'want', 'to', 'quickly', 'introduce', 'you', 'to', 'our', 'education', 'partners', 'of', 'todays', 'episode,', 'Scaler', 'School', 'of', 'Business'],\n",
    "    ['They', 'are', 'bringing', 'something', 'absolutely', 'revolutionary'],\n",
    "    ['If', 'youre', 'seeking', 'business', 'education,', 'Scaler', 'is', 'offering', 'a', 'full-time', 'on-campus', '18-month', 'PG', 'program', 'in', 'management', 'and', 'technology'],\n",
    "    ['This', 'program', 'is', 'designed', 'by', 'leaders', 'who', 'have', 'built', 'billion-dollar', 'businesses', 'like', 'Uber,', 'Myntra,', 'and', 'Meesho'],\n",
    "    ['Today,', 'legacy', 'college', 'names', 'are', 'useless', 'without', 'competitive', 'skill', 'sets'],\n",
    "    ['This', 'is', 'why,', 'as', 'part', 'of', 'the', 'Scaler', 'School', 'of', 'Business', 'curriculum,', 'you', 'will', 'work', 'on', 'real-world', 'projects', 'sourced', 'from', 'real', 'companies'],\n",
    "    ['You', 'will', 'build', 'your', 'own', 'business,', 'take', 'it', 'to', 'the', 'market,', 'generate', 'revenue,', 'and', 'raise', 'VC', 'capital', 'while', 'studying', 'on', 'campus'],\n",
    "    ['Since', 'Scaler', 'has', 'been', 'in', 'the', 'education', 'industry', 'for', 'seven', 'long', 'years,', 'they', 'already', 'have', 'access', 'to', '1,200+', 'company', 'partners,', 'something', 'no', 'other', 'B-School', 'in', 'the', 'country', 'can', 'offer', 'right', 'now'],\n",
    "    ['For', 'their', 'online', 'programs,', 'theyve', 'seen', 'placement', 'rates', 'of', '96%', 'with', 'a', 'median', 'CTC', 'of', '25', 'lakh', 'rupees,', 'verified', 'by', 'B2K', 'Analytics,', 'the', 'same', 'partner', 'that', 'verifies', 'reports', 'for', 'IIMs'],\n",
    "    ['Now,', 'Scaler', 'School', 'of', 'Business', 'is', 'handpicking', 'only', '75', 'students', 'for', 'their', 'founding', 'cohort', 'starting', 'in', 'August', '2024'],\n",
    "    ['They', 'have', 'scholarships', 'up', 'to', '100%,', 'and', 'applications', 'are', 'open', 'right', 'now', 'If', 'you', 'want', 'to', 'become', 'a', 'brilliant', 'business', 'leader,', 'apply', 'for', 'Scaler', 'School', 'of', 'Business', 'using', 'the', 'link', 'in', 'my', 'description', 'and', 'in', 'the', 'comment', 'section'],\n",
    "    ['Now,', 'on', 'with', 'the', 'episode'],\n",
    "    ['The', 'first', 'thing', 'you', 'need', 'to', 'understand', 'about', 'a', 'company', 'is', 'their', 'philosophy', 'of', 'success'],\n",
    "    ['In', 'this', 'case,', 'you', 'need', 'to', 'understand', 'what', 'exactly', 'is', 'the', 'philosophy', 'that', 'turned', 'Hyundai', 'into', 'what', 'it', 'is', 'today', 'in', 'India'],\n",
    "    ['Like', 'we', 'saw', 'in', 'the', 'Boeing', 'video', 'no', 'matter', 'how', 'much', 'profitability,', 'EBITDA,', 'or', 'earnings', 'per', 'share', 'a', 'company', 'has', 'if', 'it', 'deviates', 'from', 'its', 'core', 'philosophy', 'all', 'these', 'numbers', 'will', 'go', 'for', 'a', 'toss'],\n",
    "    ['So', 'lets', 'understand', 'the', 'core', 'philosophy', 'and', 'then', 'move', 'on', 'to', 'profitability,', 'EBITDA,', 'and', 'other', 'numbers'],\n",
    "    ['Now', 'the', 'question', 'is,', 'what', 'exactly', 'is', 'the', 'secret', 'recipe', 'for', 'Hyundai', 'success', 'in', 'India?'],\n",
    "    ['This', 'story', 'dates', 'back', 'to', 'the', 'early', '1990s'],\n",
    "    ['Until', 'this', 'point,', 'India', 'only', 'had', 'a', 'few', 'car', 'brands:', 'Hindustan', 'Ambassador,', 'Contessa,', 'Premier', 'Padmini,', 'Standard', 'Herald,', '2000,', 'Maruti', '800,', 'Omni,', 'and', 'Gypsy']\n",
    "    ],\n",
    "    \"label\": [\n",
    "        ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC'],\n",
    "['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC'],\n",
    "['B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC'],\n",
    "['B-ORG', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'B-LOC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'O', 'O'],\n",
    "['O', 'O', 'O', 'B-ORG', 'O', 'O', 'B-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'B-MISC', 'B-MISC', 'O', 'B-MISC', 'O', 'O', 'O', 'O'],\n",
    "['O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'o', 'B-MISC'],\n",
    "['O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'B-MISC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-ORG', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'O', 'B-ORG'],\n",
    "['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-ORG'],\n",
    "['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC'],\n",
    "['O', 'O', 'B-MISC', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'B-LOC'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'B-MISC', 'B-MISC', 'O', 'B-MISC', 'O', 'B-MISC', 'O', 'B-MISC', 'B-MISC', 'B-MISC', 'O', 'B-MISC']\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dslim/bert-base-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 39/39 [00:00<00:00, 1770.42 examples/s]\n",
      "C:\\Users\\vidya\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 3/9 [02:01<03:42, 37.06s/it]\n",
      " 33%|███▎      | 3/9 [02:54<03:42, 37.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7713673710823059, 'eval_runtime': 53.0025, 'eval_samples_per_second': 0.736, 'eval_steps_per_second': 0.057, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [04:55<02:18, 46.12s/it]\n",
      " 67%|██████▋   | 6/9 [05:54<02:18, 46.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5407052040100098, 'eval_runtime': 59.0046, 'eval_samples_per_second': 0.661, 'eval_steps_per_second': 0.051, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [07:54<00:00, 48.75s/it]\n",
      "100%|██████████| 9/9 [08:50<00:00, 58.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.482287734746933, 'eval_runtime': 54.0961, 'eval_samples_per_second': 0.721, 'eval_steps_per_second': 0.055, 'epoch': 3.0}\n",
      "{'train_runtime': 530.2797, 'train_samples_per_second': 0.221, 'train_steps_per_second': 0.017, 'train_loss': 0.8417130576239692, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9, training_loss=0.8417130576239692, metrics={'train_runtime': 530.2797, 'train_samples_per_second': 0.221, 'train_steps_per_second': 0.017, 'total_flos': 30572549729280.0, 'train_loss': 0.8417130576239692, 'epoch': 3.0})"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "label_list = ['O', 'B-ORG', 'I-ORG', 'B-LOC', 'B-MISC']\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('dslim/bert-base-NER')\n",
    "\n",
    "# Load pre-trained model with custom label mapping\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'dslim/bert-base-NER',\n",
    "    num_labels=len(label_list),\n",
    "    ignore_mismatched_sizes=True,\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    label2id={label: i for i, label in enumerate(label_list)}\n",
    ")\n",
    "\n",
    "# Tokenize the text and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"label\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_list.index(label[word_idx].upper()))\n",
    "            else:\n",
    "                label_ids.append(label_list.index(label[word_idx].upper()) if label_list.index(label[word_idx].upper()) % 2 == 1 else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = datase.map(tokenize_and_align_labels, batched=True, remove_columns=datase.column_names)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Amazon, Label: B-LOC, Confidence: 0.3219\n",
      "Entity: India, Label: B-LOC, Confidence: 0.2813\n",
      "Entity: Mar, Label: B-ORG, Confidence: 0.2598\n",
      "Entity: ##uti, Label: I-ORG, Confidence: 0.3691\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./Desktop')\n",
    "tokenizer.save_pretrained('./Desktop')\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./Desktop')\n",
    "model = AutoModelForTokenClassification.from_pretrained('./Desktop')\n",
    "\n",
    "# Create a NER pipeline\n",
    "ner_pipeline = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "# Example sentence\n",
    "example_sentence = \"Amazon plans to invest $2 billion in India and Maruti.\"\n",
    "\n",
    "# Use the NER pipeline to predict entities\n",
    "ner_results = ner_pipeline(example_sentence)\n",
    "\n",
    "# Print the results\n",
    "for entity in ner_results:\n",
    "    print(f'Entity: {entity['word']}, Label: {entity['entity']}, Confidence: {entity['score']:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'everybody!', 'Hyundai', 'is', 'one', 'of', 'the', 'most', 'iconic', 'companies', 'to', 'enter', 'the', 'Indian', 'markets']:['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['It', 'is', 'hard', 'to', 'believe', 'that', 'a', 'South', 'Korean', 'brand,', 'which', 'was', 'once', 'a', 'difficult', 'name', 'to', 'pronounce,', 'is', 'today', 'about', 'to', 'launch', 'the', 'biggest', 'IPO', 'in', 'the', 'history', 'of', 'India']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC']\n",
      "['Indian', 'stock', 'markets', 'could', 'soon', 'see', 'their', 'biggest', 'ever', 'IPO']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC']\n",
      "['South', 'Korean', 'automaker', 'Hyundai', 'is', 'gearing', 'up', 'for', 'a', 'significant', 'milestone', 'by', 'going', 'public', 'in', 'India']:['B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n",
      "['Hyundais', 'IPO', 'is', 'poised', 'to', 'be', 'one', 'of', 'the', 'largest', 'in', 'recent', 'times,', 'potentially', 'the', 'biggest', 'IPO', 'ever', 'in', 'India']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n",
      "['This', 'would', 'be', 'Indias', 'largest', 'IPO', 'and', 'the', 'first', 'van', 'automaker', 'since', 'Maruti', 'Suzuki', 'in', '2003']:['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['In', '1996,', 'when', 'Hyundai', 'entered', 'the', 'Indian', 'market,', 'Maruti', 'was', 'the', 'most', 'dominant', 'player', 'with', 'a', '60%', 'market', 'share,', 'almost', 'a', 'monopoly', 'amongst', 'the', 'middle', 'class', 'in', 'India']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-LOC']\n",
      "['Despite', 'this,', 'Hyundai', 'came', 'in', 'with', 'models', 'like', 'Santro,', 'i10,', 'i20,', 'and', 'Creta,', 'making', 'a', 'significant', 'impact']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O']\n",
      "['Today,', 'Hyundai', 'is', 'the', 'second-largest', 'automaker', 'in', 'the', 'country', 'with', 'a', 'profit', 'of', '4,653', 'crores']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['In', 'terms', 'of', 'profitability,', 'Hyundai', 'is', 'way', 'ahead', 'of', 'Maruti']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['In', 'FY23,', 'while', 'Hyundai', 'made', 'a', 'profit', 'of', '65,355', 'rupees', 'per', 'vehicle,', 'Maruti', 'made', 'a', 'profit', 'of', '4,939', 'rupees', 'per', 'vehicle']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['Now,', 'after', '28', 'years', 'of', 'establishing', 'a', 'stronghold', 'in', 'the', 'Indian', 'market,', 'Hyundai', 'plans', 'to', 'raise', '25,000', 'crores', 'with', 'its', 'IPO']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O']\n",
      "['Sounds', 'fantastic,', 'right?']:['O', 'O', 'O']\n",
      "['However,', 'there', 'are', 'some', 'hidden', 'risks', 'that', 'Hyundai', 'faces', 'which', 'may', 'jeopardize', 'all', 'the', 'progress', 'theyve', 'made', 'in', 'India']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['All', 'of', 'this', 'is', 'detailed', 'in', 'a', '436-page', 'DRHP', 'paper', 'that', 'Hyundai', 'filed', 'with', 'SEBI']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['Weve', 'gone', 'through', 'all', 'this', 'data,', 'so', 'that', 'you', 'can', 'sit', 'back', 'and', 'consume', 'the', 'most', 'important', 'bits', 'of', 'this', '400-page', 'document', 'as', 'easily', 'as', 'watching', 'a', 'movie']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "['In', 'this', 'episode,', 'lets', 'dive', 'into', 'understanding', 'how', 'a', 'South', 'Korean', 'company', 'like', 'Hyundai', 'broke', 'into', 'the', 'Indian', 'automobile', 'market,', 'leaving', 'behind', 'giants', 'like', 'Tata', 'and', 'Mahindra']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['Despite', 'being', 'a', 'foreign', 'company,', 'how', 'did', 'Hyundai', 'achieve', 'such', 'high', 'profitability', 'and', 'outpace', 'a', 'mammoth', 'like', 'Maruti', 'in', 'India?']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O']\n",
      "['Most', 'importantly,', 'after', 'making', 'such', 'amazing', 'progress,', 'what', 'are', 'the', 'hidden', 'risks', 'that', 'could', 'topple', 'Hyundai', 'growth', 'in', 'India?']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['Also,', 'a', 'quick', 'disclaimer:', 'I', 'am', 'not', 'a', 'SEBI-registered', 'investment', 'adviser,', 'and', 'this', 'video', 'is', 'not', 'meant', 'to', 'give', 'you', 'investment', 'advice', 'but', 'to', 'only', 'educate', 'you', 'about', 'the', 'rise', 'of', 'Hyundai', 'in', 'India']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['Before', 'we', 'move', 'on,', 'I', 'want', 'to', 'quickly', 'introduce', 'you', 'to', 'our', 'education', 'partners', 'of', 'todays', 'episode,', 'Scaler', 'School', 'of', 'Business']:['O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['They', 'are', 'bringing', 'something', 'absolutely', 'revolutionary']:['O', 'O', 'O', 'O', 'O', 'O']\n",
      "['If', 'youre', 'seeking', 'business', 'education,', 'Scaler', 'is', 'offering', 'a', 'full-time', 'on-campus', '18-month', 'PG', 'program', 'in', 'management', 'and', 'technology']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['This', 'program', 'is', 'designed', 'by', 'leaders', 'who', 'have', 'built', 'billion-dollar', 'businesses', 'like', 'Uber,', 'Myntra,', 'and', 'Meesho']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['Today,', 'legacy', 'college', 'names', 'are', 'useless', 'without', 'competitive', 'skill', 'sets']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['This', 'is', 'why,', 'as', 'part', 'of', 'the', 'Scaler', 'School', 'of', 'Business', 'curriculum,', 'you', 'will', 'work', 'on', 'real-world', 'projects', 'sourced', 'from', 'real', 'companies']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['You', 'will', 'build', 'your', 'own', 'business,', 'take', 'it', 'to', 'the', 'market,', 'generate', 'revenue,', 'and', 'raise', 'VC', 'capital', 'while', 'studying', 'on', 'campus']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "['Since', 'Scaler', 'has', 'been', 'in', 'the', 'education', 'industry', 'for', 'seven', 'long', 'years,', 'they', 'already', 'have', 'access', 'to', '1,200+', 'company', 'partners,', 'something', 'no', 'other', 'B-School', 'in', 'the', 'country', 'can', 'offer', 'right', 'now']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['For', 'their', 'online', 'programs,', 'theyve', 'seen', 'placement', 'rates', 'of', '96%', 'with', 'a', 'median', 'CTC', 'of', '25', 'lakh', 'rupees,', 'verified', 'by', 'B2K', 'Analytics,', 'the', 'same', 'partner', 'that', 'verifies', 'reports', 'for', 'IIMs']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-ORG', 'O']\n",
      "['Now,', 'Scaler', 'School', 'of', 'Business', 'is', 'handpicking', 'only', '75', 'students', 'for', 'their', 'founding', 'cohort', 'starting', 'in', 'August', '2024']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O']\n",
      "['They', 'have', 'scholarships', 'up', 'to', '100%,', 'and', 'applications', 'are', 'open', 'right', 'now', 'If', 'you', 'want', 'to', 'become', 'a', 'brilliant', 'business', 'leader,', 'apply', 'for', 'Scaler', 'School', 'of', 'Business', 'using', 'the', 'link', 'in', 'my', 'description', 'and', 'in', 'the', 'comment', 'section']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC']\n",
      "['Now,', 'on', 'with', 'the', 'episode']:['O', 'O', 'B-MISC', 'O', 'O']\n",
      "['The', 'first', 'thing', 'you', 'need', 'to', 'understand', 'about', 'a', 'company', 'is', 'their', 'philosophy', 'of', 'success']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['In', 'this', 'case,', 'you', 'need', 'to', 'understand', 'what', 'exactly', 'is', 'the', 'philosophy', 'that', 'turned', 'Hyundai', 'into', 'what', 'it', 'is', 'today', 'in', 'India']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['Like', 'we', 'saw', 'in', 'the', 'Boeing', 'video', 'no', 'matter', 'how', 'much', 'profitability,', 'EBITDA,', 'or', 'earnings', 'per', 'share', 'a', 'company', 'has', 'if', 'it', 'deviates', 'from', 'its', 'core', 'philosophy', 'all', 'these', 'numbers', 'will', 'go', 'for', 'a', 'toss']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O']\n",
      "['So', 'lets', 'understand', 'the', 'core', 'philosophy', 'and', 'then', 'move', 'on', 'to', 'profitability,', 'EBITDA,', 'and', 'other', 'numbers']:['O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O']\n",
      "['Now', 'the', 'question', 'is,', 'what', 'exactly', 'is', 'the', 'secret', 'recipe', 'for', 'Hyundai', 'success', 'in', 'India?']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['This', 'story', 'dates', 'back', 'to', 'the', 'early', '1990s']:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['Until', 'this', 'point,', 'India', 'only', 'had', 'a', 'few', 'car', 'brands:', 'Hindustan', 'Ambassador,', 'Contessa,', 'Premier', 'Padmini,', 'Standard', 'Herald,', '2000,', 'Maruti', '800,', 'Omni,', 'and', 'Gypsy']:['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(datase[\"text\"])):\n",
    "    print(f\"{datase[\"text\"][i]}:{datase[\"labels\"][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "['Hi', 'everybody!', 'Hyundai', 'is', 'one', 'of', 'the', 'most', 'iconic', 'companies', 'to', 'enter', 'the', 'Indian', 'markets']:\n",
    "['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']\n",
    "['It', 'is', 'hard', 'to', 'believe', 'that', 'a', 'South', 'Korean', 'brand,', 'which', 'was', 'once', 'a', 'difficult', 'name', 'to', 'pronounce,', 'is', 'today', 'about', 'to', 'launch', 'the', 'biggest', 'IPO', 'in', 'the', 'history', 'of', 'India']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC']\n",
    "['Indian', 'stock', 'markets', 'could', 'soon', 'see', 'their', 'biggest', 'ever', 'IPO']:\n",
    "['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC']\n",
    "['South', 'Korean', 'automaker', 'Hyundai', 'is', 'gearing', 'up', 'for', 'a', 'significant', 'milestone', 'by', 'going', 'public', 'in', 'India']:\n",
    "['B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n",
    "['Hyundais', 'IPO', 'is', 'poised', 'to', 'be', 'one', 'of', 'the', 'largest', 'in', 'recent', 'times,', 'potentially', 'the', 'biggest', 'IPO', 'ever', 'in', 'India']:\n",
    "['B-ORG', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-LOC']\n",
    "['This', 'would', 'be', 'Indias', 'largest', 'IPO', 'and', 'the', 'first', 'van', 'automaker', 'since', 'Maruti', 'Suzuki', 'in', '2003']:\n",
    "['O', 'O', 'O', 'B-LOC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'O', 'O']\n",
    "['In', '1996,', 'when', 'Hyundai', 'entered', 'the', 'Indian', 'market,', 'Maruti', 'was', 'the', 'most', 'dominant', 'player', 'with', 'a', '60%', 'market', 'share,', 'almost', 'a', 'monopoly', 'amongst', 'the', 'middle', 'class', 'in', 'India']:\n",
    "['O', 'O', 'O', 'B-ORG', 'O', 'O', 'B-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-LOC']\n",
    "['Despite', 'this,', 'Hyundai', 'came', 'in', 'with', 'models', 'like', 'Santro,', 'i10,', 'i20,', 'and', 'Creta,', 'making', 'a', 'significant', 'impact']:\n",
    "['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'B-MISC', 'B-MISC', 'O', 'B-MISC', 'O', 'O', 'O', 'O']\n",
    "['Today,', 'Hyundai', 'is', 'the', 'second-largest', 'automaker', 'in', 'the', 'country', 'with', 'a', 'profit', 'of', '4,653', 'crores']:\n",
    "['O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "['In', 'terms', 'of', 'profitability,', 'Hyundai', 'is', 'way', 'ahead', 'of', 'Maruti']:\n",
    "['O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O']\n",
    "['In', 'FY23,', 'while', 'Hyundai', 'made', 'a', 'profit', 'of', '65,355', 'rupees', 'per', 'vehicle,', 'Maruti', 'made', 'a', 'profit', 'of', '4,939', 'rupees', 'per', 'vehicle']:\n",
    "['O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "['Now,', 'after', '28', 'years', 'of', 'establishing', 'a', 'stronghold', 'in', 'the', 'Indian', 'market,', 'Hyundai', 'plans', 'to', 'raise', '25,000', 'crores', 'with', 'its', 'IPO']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'o', 'B-MISC']\n",
    "['Sounds', 'fantastic,', 'right?']:\n",
    "['O', 'O', 'O']\n",
    "['However,', 'there', 'are', 'some', 'hidden', 'risks', 'that', 'Hyundai', 'faces', 'which', 'may', 'jeopardize', 'all', 'the', 'progress', 'theyve', 'made', 'in', 'India']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n",
    "['All', 'of', 'this', 'is', 'detailed', 'in', 'a', '436-page', 'DRHP', 'paper', 'that', 'Hyundai', 'filed', 'with', 'SEBI']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'B-MISC']\n",
    "['Weve', 'gone', 'through', 'all', 'this', 'data,', 'so', 'that', 'you', 'can', 'sit', 'back', 'and', 'consume', 'the', 'most', 'important', 'bits', 'of', 'this', '400-page', 'document', 'as', 'easily', 'as', 'watching', 'a', 'movie']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "['In', 'this', 'episode,', 'lets', 'dive', 'into', 'understanding', 'how', 'a', 'South', 'Korean', 'company', 'like', 'Hyundai', 'broke', 'into', 'the', 'Indian', 'automobile', 'market,', 'leaving', 'behind', 'giants', 'like', 'Tata', 'and', 'Mahindra']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG']\n",
    "['Despite', 'being', 'a', 'foreign', 'company,', 'how', 'did', 'Hyundai', 'achieve', 'such', 'high', 'profitability', 'and', 'outpace', 'a', 'mammoth', 'like', 'Maruti', 'in', 'India']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-ORG', 'O', 'B-LOC']\n",
    "['Most', 'importantly,', 'after', 'making', 'such', 'amazing', 'progress,', 'what', 'are', 'the', 'hidden', 'risks', 'that', 'could', 'topple', 'Hyundai', 'growth', 'in', 'India']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'B-LOC']\n",
    "['Also,', 'a', 'quick', 'disclaimer:', 'I', 'am', 'not', 'a', 'SEBI-registered', 'investment', 'adviser,', 'and', 'this', 'video', 'is', 'not', 'meant', 'to', 'give', 'you', 'investment', 'advice', 'but', 'to', 'only', 'educate', 'you', 'about', 'the', 'rise', 'of', 'Hyundai', 'in', 'India']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-LOC']\n",
    "['Before', 'we', 'move', 'on,', 'I', 'want', 'to', 'quickly', 'introduce', 'you', 'to', 'our', 'education', 'partners', 'of', 'todays', 'episode,', 'Scaler', 'School', 'of', 'Business']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
    "['They', 'are', 'bringing', 'something', 'absolutely', 'revolutionary']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O']\n",
    "['If', 'youre', 'seeking', 'business', 'education,', 'Scaler', 'is', 'offering', 'a', 'full-time', 'on-campus', '18-month', 'PG', 'program', 'in', 'management', 'and', 'technology']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "['This', 'program', 'is', 'designed', 'by', 'leaders', 'who', 'have', 'built', 'billion-dollar', 'businesses', 'like', 'Uber,', 'Myntra,', 'and', 'Meesho']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'O', 'B-ORG']\n",
    "['Today,', 'legacy', 'college', 'names', 'are', 'useless', 'without', 'competitive', 'skill', 'sets']:\n",
    "['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "['This', 'is', 'why,', 'as', 'part', 'of', 'the', 'Scaler', 'School', 'of', 'Business', 'curriculum,', 'you', 'will', 'work', 'on', 'real-world', 'projects', 'sourced', 'from', 'real', 'companies']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "['You', 'will', 'build', 'your', 'own', 'business,', 'take', 'it', 'to', 'the', 'market,', 'generate', 'revenue,', 'and', 'raise', 'VC', 'capital', 'while', 'studying', 'on', 'campus']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
    "['Since', 'Scaler', 'has', 'been', 'in', 'the', 'education', 'industry', 'for', 'seven', 'long', 'years,', 'they', 'already', 'have', 'access', 'to', '1,200+', 'company', 'partners,', 'something', 'no', 'other', 'B-School', 'in', 'the', 'country', 'can', 'offer', 'right', 'now']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "['For', 'their', 'online', 'programs,', 'theyve', 'seen', 'placement', 'rates', 'of', '96%', 'with', 'a', 'median', 'CTC', 'of', '25', 'lakh', 'rupees,', 'verified', 'by', 'B2K', 'Analytics,', 'the', 'same', 'partner', 'that', 'verifies', 'reports', 'for', 'IIMs']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-ORG']\n",
    "['Now,', 'Scaler', 'School', 'of', 'Business', 'is', 'handpicking', 'only', '75', 'students', 'for', 'their', 'founding', 'cohort', 'starting', 'in', 'August', '2024']:\n",
    "['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O']\n",
    "['They', 'have', 'scholarships', 'up', 'to', '100%,', 'and', 'applications', 'are', 'open', 'right', 'now', 'If', 'you', 'want', 'to', 'become', 'a', 'brilliant', 'business', 'leader,', 'apply', 'for', 'Scaler', 'School', 'of', 'Business', 'using', 'the', 'link', 'in', 'my', 'description', 'and', 'in', 'the', 'comment', 'section']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC']\n",
    "['Now,', 'on', 'with', 'the', 'episode']:\n",
    "['O', 'O', 'B-MISC', 'O', 'O']\n",
    "['The', 'first', 'thing', 'you', 'need', 'to', 'understand', 'about', 'a', 'company', 'is', 'their', 'philosophy', 'of', 'success']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "['In', 'this', 'case,', 'you', 'need', 'to', 'understand', 'what', 'exactly', 'is', 'the', 'philosophy', 'that', 'turned', 'Hyundai', 'into', 'what', 'it', 'is', 'today', 'in', 'India']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n",
    "['Like', 'we', 'saw', 'in', 'the', 'Boeing', 'video', 'no', 'matter', 'how', 'much', 'profitability,', 'EBITDA,', 'or', 'earnings', 'per', 'share', 'a', 'company', 'has', 'if', 'it', 'deviates', 'from', 'its', 'core', 'philosophy', 'all', 'these', 'numbers', 'will', 'go', 'for', 'a', 'toss']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "['So', 'lets', 'understand', 'the', 'core', 'philosophy', 'and', 'then', 'move', 'on', 'to', 'profitability,', 'EBITDA,', 'and', 'other', 'numbers']:\n",
    "['O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O']\n",
    "['Now', 'the', 'question', 'is,', 'what', 'exactly', 'is', 'the', 'secret', 'recipe', 'for', 'Hyundai', 'success', 'in', 'India']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'B-LOC']\n",
    "['This', 'story', 'dates', 'back', 'to', 'the', 'early', '1990s']:\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "['Until', 'this', 'point,', 'India', 'only', 'had', 'a', 'few', 'car', 'brands:', 'Hindustan', 'Ambassador,', 'Contessa,', 'Premier', 'Padmini,', 'Standard', 'Herald,', '2000,', 'Maruti', '800,', 'Omni,', 'and', 'Gypsy']:\n",
    "['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'B-MISC', 'B-MISC', 'O', 'B-MISC', 'O', 'B-MISC', 'O', 'B-MISC', 'B-MISC', 'B-MISC', 'O', 'B-MISC']\n",
    "\n",
    "\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
